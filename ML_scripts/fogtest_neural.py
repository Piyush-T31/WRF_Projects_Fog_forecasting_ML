# -*- coding: utf-8 -*-
"""FogTest_neural

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HrlAi4GeWsRg-wZiB5ylRh99dDgBEL4z
"""

#@title Import relevant modules
#!pip install scikeras
!pip install imbalanced-learn
!pip install dask
#from scikeras.wrappers import KerasClassifier
import numpy as np
import pandas as pd
import tensorflow as tf
from collections import Counter
from tensorflow import keras
from sklearn.preprocessing import StandardScaler, LabelEncoder
from keras.utils import to_categorical
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout, Bidirectional, GRU, BatchNormalization, LeakyReLU, PReLU, ELU, ReLU, Input
from tensorflow.keras.activations import tanh
from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, Flatten, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.metrics import Precision, Recall, Accuracy, AUC
from tensorflow.keras.initializers import Constant
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import MinMaxScaler
from sklearn.utils import resample
from sklearn.utils.class_weight import compute_class_weight
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import BorderlineSMOTE, SMOTE
import matplotlib.pyplot as plt
import seaborn as sns
import glob
import dask.dataframe as dd

# The following lines adjust the granularity of reporting.
pd.options.display.max_rows = 10
pd.options.display.float_format = "{:.1f}".format

print("Imported modules.")

"""# Using LSTM for all locations"""

# @title Cleaning the Data
# Load the CSV file into a DataFrame
# Load and clean data
def load_and_clean_data(path):
    df = pd.read_csv(path)
    df = df[df['Vis'].notnull()]
    return df

location_data = load_and_clean_data('SableCombined_era5.csv')
location1_data = load_and_clean_data('JohnCombined_era5.csv')
location2_data = load_and_clean_data('YarmouthCombined_era5.csv')

# Ensure the time columns are in datetime format
location_data['Time'] = pd.to_datetime(location_data['Time'], format='%Y-%m-%d_%H:%M:%S')
location1_data['Time'] = pd.to_datetime(location1_data['Time'], format='%Y-%m-%d_%H:%M:%S')
location2_data['Time'] = pd.to_datetime(location2_data['Time'], format='%Y-%m-%d_%H:%M:%S')

# Add location identifiers
location_data['Location'] = 'Sable'
location1_data['Location'] = 'StJohn'
location2_data['Location'] = 'Yarmouth'

# Combine the datasets
combined_data = pd.concat([location_data, location1_data, location2_data])
combined_data.to_csv('combined_data.csv', index=False)

# One-hot encode the location column
combined_data = pd.get_dummies(combined_data, columns=['Location'], drop_first=False)

# Encode 'class_vis' column
label_encoder = LabelEncoder()
combined_data['class_vis'] = label_encoder.fit_transform(combined_data['class_vis'])

# Sort the data by time
combined_data = combined_data.sort_values(by='Time')

# Create lagged features
def create_lagged_features(data, lag=1):
    lagged_data = data.copy()
    for i in range(1, lag + 1):
        shifted = data.shift(i)
        shifted.columns = [f'{col}_lag{i}' for col in data.columns]
        lagged_data = pd.concat([lagged_data, shifted], axis=1)
    lagged_data.dropna(inplace=True)
    return lagged_data

lag = 2  # You can adjust this based on your data and requirements
lagged_data = create_lagged_features(combined_data.drop(['Time', 'class_vis'], axis=1), lag=lag)
lagged_data['class_vis'] = combined_data['class_vis'][lag:]

# Split into features and target
X = lagged_data.drop(['class_vis','Vis'], axis=1)
y = lagged_data['class_vis']
# Preserve the location information before preprocessing
locations = lagged_data[['Location_Sable', 'Location_StJohn', 'Location_Yarmouth']]

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Determine the index to split the data
split_index = int(len(X_scaled) * 0.8)

# Split the data into train and test sets
X_train, X_test = X_scaled[:split_index], X_scaled[split_index:]
y_train, y_test = y[:split_index], y[split_index:]

# Split the locations into train and test sets
locations_train = locations[:split_index]
locations_test = locations[split_index:]

# Apply SMOTE to the training data only
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Reshape data for LSTM
n_features = X_train_resampled.shape[1] // (lag + 2)  # Calculate the correct number of features per lag

X_train_resampled = X_train_resampled.reshape((X_train_resampled.shape[0], lag + 2, n_features))
X_test = X_test.reshape((X_test.shape[0], lag + 2, n_features))
y_train_resampled = np.array(y_train_resampled)
y_test = np.array(y_test)

# Compute class weights
class_weights = compute_class_weight('balanced', classes=np.unique(y_train_resampled), y=y_train_resampled)
class_weights = {i: class_weights[i] for i in range(len(class_weights))}

# @title Building the Model

model = Sequential()
model.add(Bidirectional(LSTM(256, activation='relu', return_sequences=True), input_shape=(X_train_resampled.shape[1], X_train_resampled.shape[2])))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Bidirectional(LSTM(512, activation='relu')))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(len(np.unique(y)), activation='softmax'))

# Set the initial learning rate
learning_rate = 0.00001

# Compile the model
model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.000001)
history = model.fit(X_train_resampled, y_train_resampled, epochs=20, batch_size=10, validation_split=0.2, callbacks=[early_stopping, reduce_lr], class_weight=class_weights)


# Plotting the results
plt.figure(figsize=(14, 6))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='val_accuracy')
plt.legend()
plt.title('Model Accuracy')

# Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label='val_loss')
plt.legend()
plt.title('Model Loss')

plt.show()

# @title Prediction

# Function to plot confusion matrix
def plot_confusion_matrix(conf_matrix, classes, title='Confusion Matrix'):
    plt.figure(figsize=(10, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title(title)
    plt.show()

# Predict on the test set
predictions = model.predict(X_test)
predicted_classes = np.argmax(predictions, axis=1)
true_classes = y_test

# Convert numeric classes back to original labels
predicted_labels = label_encoder.inverse_transform(predicted_classes)
true_labels = label_encoder.inverse_transform(true_classes)

# Generate and print overall classification report
report = classification_report(true_labels, predicted_labels, target_names=label_encoder.classes_)
print("Overall Classification Report:")
print(report)

# Generate and plot overall confusion matrix
conf_matrix = confusion_matrix(true_labels, predicted_labels)
plot_confusion_matrix(conf_matrix, classes=label_encoder.classes_, title='Overall Confusion Matrix')

# Combine test data with predictions for analysis by location
test_data_with_predictions = pd.DataFrame({
    'Location_Sable': locations_test['Location_Sable'],
    'Location_StJohn': locations_test['Location_StJohn'],
    'Location_Yarmouth': locations_test['Location_Yarmouth'],
    'True': true_labels,
    'Predicted': predicted_labels
})

# Evaluate and plot confusion matrix for each location
location_columns = ['Location_Sable', 'Location_StJohn', 'Location_Yarmouth']
location_names = ['Sable', 'StJohn', 'Yarmouth']

for location_column, location_name in zip(location_columns, location_names):
    location_data = test_data_with_predictions[test_data_with_predictions[location_column] == 1]
    true_labels_location = location_data['True']
    predicted_labels_location = location_data['Predicted']

    # Generate classification report
    report = classification_report(true_labels_location, predicted_labels_location, target_names=label_encoder.classes_)
    print(f"Classification Report for {location_name}:")
    print(report)

    # Generate confusion matrix
    conf_matrix = confusion_matrix(true_labels_location, predicted_labels_location)

    # Plot the confusion matrix
    plot_confusion_matrix(conf_matrix, classes=label_encoder.classes_, title=f'Confusion Matrix for {location_name}')

"""# Building the GRU Model"""

# @title Cleaning the Data
# Load the CSV file into a DataFrame
# Load and clean data
def load_and_clean_data(path):
    df = pd.read_csv(path)
    df = df[df['Vis'].notnull()]
    return df

location_data = load_and_clean_data('SableCombined_era5.csv')
location1_data = load_and_clean_data('JohnCombined_era5.csv')
location2_data = load_and_clean_data('YarmouthCombined_era5.csv')

# Ensure the time columns are in datetime format
location_data['Time'] = pd.to_datetime(location_data['Time'], format='%Y-%m-%d_%H:%M:%S')
location1_data['Time'] = pd.to_datetime(location1_data['Time'], format='%Y-%m-%d_%H:%M:%S')
location2_data['Time'] = pd.to_datetime(location2_data['Time'], format='%Y-%m-%d_%H:%M:%S')

# Add location identifiers
location_data['Location'] = 'Sable'
location1_data['Location'] = 'StJohn'
location2_data['Location'] = 'Yarmouth'

# Combine the datasets
combined_data = pd.concat([location_data, location1_data, location2_data])
combined_data.to_csv('combined_data.csv', index=False)

# One-hot encode the location column
combined_data = pd.get_dummies(combined_data, columns=['Location'], drop_first=False)
# Encode 'class_vis' column
label_encoder = LabelEncoder()
combined_data['class_vis'] = label_encoder.fit_transform(combined_data['class_vis'])

# Preserve the location information before preprocessing
locations = combined_data[['Location_Sable', 'Location_StJohn', 'Location_Yarmouth']]

# Split into features and target
X = combined_data.drop(['Time', 'class_vis', 'Vis'], axis=1)
y = combined_data['class_vis']

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into train and test sets
X_train, X_test, y_train, y_test, locations_train, locations_test = train_test_split(
    X_scaled, y, locations, test_size=0.2, stratify=y, random_state=42
)
# Apply SMOTE to the training data only
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Reshape data for GRU
X_train_resampled = X_train_resampled.reshape((X_train_resampled.shape[0], X_train_resampled.shape[1], 1))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

# Compute class weights
class_weights = compute_class_weight('balanced', classes=np.unique(y_train_resampled), y=y_train_resampled)
class_weights = {i: class_weights[i] for i in range(len(class_weights))}

# @title Build a simplified GRU model
model = Sequential()
model.add(GRU(256, activation='relu', return_sequences=True, input_shape=(X_train_resampled.shape[1], X_train_resampled.shape[2])))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(GRU(512, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(len(np.unique(y)), activation='softmax'))

# Compile and train the model as before
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
history = model.fit(X_train_resampled, y_train_resampled, epochs=200, batch_size=24, validation_split=0.2, callbacks=[early_stopping], class_weight=class_weights)

# Plot training history
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label='val_loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='val_accuracy')
plt.legend()
plt.show()

# @title Evaluate the performance

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Function to plot confusion matrix
def plot_confusion_matrix(conf_matrix, classes, title='Confusion Matrix'):
    plt.figure(figsize=(10, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title(title)
    plt.show()

# Predict on the test set
predictions = model.predict(X_test)
predicted_classes = np.argmax(predictions, axis=1)
true_classes = y_test

# Convert numeric classes back to original labels
predicted_labels = label_encoder.inverse_transform(predicted_classes)
true_labels = label_encoder.inverse_transform(true_classes)

# Generate and print overall classification report
report = classification_report(true_labels, predicted_labels, target_names=label_encoder.classes_)
print("Overall Classification Report:")
print(report)

# Generate and plot overall confusion matrix
conf_matrix = confusion_matrix(true_labels, predicted_labels)
plot_confusion_matrix(conf_matrix, classes=label_encoder.classes_, title='Overall Confusion Matrix')

# Combine test data with predictions for analysis by location
test_data_with_predictions = pd.DataFrame({
    'Location_Sable': locations_test['Location_Sable'],
    'Location_StJohn': locations_test['Location_StJohn'],
    'Location_Yarmouth': locations_test['Location_Yarmouth'],
    'True': true_labels,
    'Predicted': predicted_labels
})

# Evaluate and plot confusion matrix for each location
location_columns = ['Location_Sable', 'Location_StJohn', 'Location_Yarmouth']
location_names = ['Sable', 'StJohn', 'Yarmouth']

for location_column, location_name in zip(location_columns, location_names):
    location_data = test_data_with_predictions[test_data_with_predictions[location_column] == 1]
    true_labels_location = location_data['True']
    predicted_labels_location = location_data['Predicted']

    # Generate classification report
    report = classification_report(true_labels_location, predicted_labels_location, target_names=label_encoder.classes_)
    print(f"Classification Report for {location_name}:")
    print(report)

    # Generate confusion matrix
    conf_matrix = confusion_matrix(true_labels_location, predicted_labels_location)

    # Plot the confusion matrix
    plot_confusion_matrix(conf_matrix, classes=label_encoder.classes_, title=f'Confusion Matrix for {location_name}')

"""# Running on Graham"""

# @title Running on Graham
from tensorflow.keras.models import load_model
import pickle

# Load the training history
with open('training_history.pkl', 'rb') as file:
    history = pickle.load(file)

# Plot training & validation accuracy values
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(history['accuracy'])
plt.plot(history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history['loss'])
plt.plot(history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.show()

# Load the saved model
model = load_model('lstm_model.h5')

# Function to plot confusion matrix
def plot_confusion_matrix(conf_matrix, classes, title='Confusion Matrix'):
    plt.figure(figsize=(10, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title(title)
    plt.show()

# Predict on the test set
predictions = model.predict(X_test)
predicted_classes = np.argmax(predictions, axis=1)
true_classes = y_test

# Convert numeric classes back to original labels
predicted_labels = label_encoder.inverse_transform(predicted_classes)
true_labels = label_encoder.inverse_transform(true_classes)

# Generate and print overall classification report
report = classification_report(true_labels, predicted_labels, target_names=label_encoder.classes_)
print("Overall Classification Report:")
print(report)

# Generate and plot overall confusion matrix
conf_matrix = confusion_matrix(true_labels, predicted_labels)
plot_confusion_matrix(conf_matrix, classes=label_encoder.classes_, title='Overall Confusion Matrix')

# Combine test data with predictions for analysis by location
test_data_with_predictions = pd.DataFrame({
    'Location_Sable': locations_test['Location_Sable'],
    'Location_StJohn': locations_test['Location_StJohn'],
    'Location_Yarmouth': locations_test['Location_Yarmouth'],
    'True': true_labels,
    'Predicted': predicted_labels
})

# Evaluate and plot confusion matrix for each location
location_columns = ['Location_Sable', 'Location_StJohn', 'Location_Yarmouth']
location_names = ['Sable', 'StJohn', 'Yarmouth']

for location_column, location_name in zip(location_columns, location_names):
    location_data = test_data_with_predictions[test_data_with_predictions[location_column] == 1]
    true_labels_location = location_data['True']
    predicted_labels_location = location_data['Predicted']

    # Generate classification report
    report = classification_report(true_labels_location, predicted_labels_location, target_names=label_encoder.classes_)
    print(f"Classification Report for {location_name}:")
    print(report)

    # Generate confusion matrix
    conf_matrix = confusion_matrix(true_labels_location, predicted_labels_location)

    # Plot the confusion matrix
    plot_confusion_matrix(conf_matrix, classes=label_encoder.classes_, title=f'Confusion Matrix for {location_name}')

import pandas as pd
from statsmodels.tsa.stattools import adfuller

# Load your dataset
file_path = 'Era5_Stjohn.csv'
data = pd.read_csv(file_path)

# Define a function to perform the ADF test
def adf_test(series, variable_name):
    result = adfuller(series.dropna())  # Drop missing values if any
    p_value = result[1]
    if p_value < 0.05:
        return f"{variable_name}: Stationary (p-value={p_value:.5f})"
    else:
        return f"{variable_name}: Non-stationary (p-value={p_value:.5f})"

# Select key variables to test
variables_to_test = ["T2", "U", "V", "RH2", "P_sfc"]

# Reduce the dataset size for ADF testing
data_subset = data.iloc[:50000, :]  # Use only the first 5000 rows

# Run the ADF test for the selected variables
adf_results_subset = {var: adf_test(data_subset[var], var) for var in variables_to_test}

# Display results
for var, result in adf_results_subset.items():
    print(result)

scaler = StandardScaler()
scaled_data = scaler.fit_transform(data[["T2", "U", "V", "RH2", "P_sfc"]])
scaled_df = pd.DataFrame(scaled_data, columns=["T2", "U", "V", "RH2", "P_sfc"])

adf_results_scaled = {var: adf_test(scaled_df[var], var) for var in scaled_df.columns}
for var, result in adf_results_scaled.items():
    print(result)

"""# Binary Classification

"""

tf.test.is_gpu_available()

# @title Plotting the activation function f
import numpy as np
import matplotlib.pyplot as plt

# Define activation functions
def tanh(x):
    return np.tanh(x)

def relu(x):
    return np.maximum(0, x)

def leaky_relu(x, alpha=0.1):
    return np.where(x > 0, x, alpha * x)

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Generate input values
x = np.linspace(-5, 5, 400)

# Compute function values
y_tanh = tanh(x)
y_sigmoid = sigmoid(x)
y_relu = relu(x)
y_leaky_relu = leaky_relu(x)

# Create figure and axes
fig, axes = plt.subplots(1, 2, figsize=(10, 5))

# Plot tanh & sigmoid
axes[0].plot(x, y_tanh, label='tanh', color='blue')
axes[0].plot(x, y_sigmoid, label='sigmoid', color='red')
axes[0].set_title("tanh & sigmoid")
axes[0].legend()
#axes[0].grid()

# Plot ReLU & Leaky ReLU
axes[1].plot(x, y_relu, label='ReLU', color='green')
axes[1].plot(x, y_leaky_relu, label='Leaky ReLU', color='purple')
axes[1].set_title("ReLU & Leaky ReLU")
axes[1].legend()
#axes[1].grid()

# Show plot
plt.tight_layout()
plt.show()

#@title Neural Network Architecture

import matplotlib.pyplot as plt
import networkx as nx

def draw_neural_net(ax, layer_sizes, recurrent=False):
    G = nx.DiGraph()
    v_spacing = 1
    h_spacing = 2
    node_positions = {}

    # Create nodes
    for layer_idx, layer_size in enumerate(layer_sizes):
        for node_idx in range(layer_size):
            x = layer_idx * h_spacing
            y = -node_idx * v_spacing
            G.add_node((layer_idx, node_idx))
            node_positions[(layer_idx, node_idx)] = (x, y)

    # Create edges (feed-forward)
    for layer_idx in range(len(layer_sizes) - 1):
        for node_idx in range(layer_sizes[layer_idx]):
            for next_node_idx in range(layer_sizes[layer_idx + 1]):
                G.add_edge((layer_idx, node_idx), (layer_idx + 1, next_node_idx))

    # Add recurrent connections (excluding output layer)
    if recurrent:
        for layer_idx in range(1, len(layer_sizes) - 1):  # Skip the output layer
            for node_idx in range(layer_sizes[layer_idx]):
                G.add_edge((layer_idx, node_idx), (layer_idx, node_idx))  # Self-loop

    # Draw the graph
    nx.draw(G, pos=node_positions, ax=ax, with_labels=False, node_size=500, edge_color="black", arrows=True)

    # Label layers
    ax.text(-0.2, -3.5, "Input Layer", fontsize=9, ha="center")
    ax.text(1.8, -3.5, "Hidden Layer", fontsize=9, ha="center")
    ax.text(3.7, -3.5, "Output Layer", fontsize=9, ha="center")

    ax.set_title("(a) Recurrent Neural Network" if recurrent else "(b) Feed-Forward Neural Network")

# Create figure
fig, axes = plt.subplots(1, 2, figsize=(10, 5))

# Feed-Forward NN
draw_neural_net(axes[1], [4, 3, 3], recurrent=False)

# Recurrent NN
draw_neural_net(axes[0], [4, 3, 3], recurrent=True)

plt.show()

# @title Cleaning the Data
# Load the CSV file into a DataFrame
# Define the path and pattern for the CSV files
path = 'Era5_Yarmouth_1KM.csv'

# Compute the Dask DataFrame to get a Pandas DataFrame
combined_df = pd.read_csv(path)
# Remove rows where the first column is blank
df_cleaned = combined_df[combined_df['Vis'].notnull()]

# Save the cleaned DataFrame back to a CSV file
df_cleaned.to_csv('cleaned_file.csv', index=False)

# @title Data Preparation
# Convert 'Time' column to datetime with the correct format
data = df_cleaned.copy()
data['Time'] = pd.to_datetime(data['Time'], format='%Y-%m-%d_%H:%M:%S', errors='coerce')

# Extract time-based features
data['Hour'] = data['Time'].dt.hour
data['Day'] = data['Time'].dt.day
data['Month'] = data['Time'].dt.month
data['DayOfWeek'] = data['Time'].dt.dayofweek

# Sort the data by time
data = data.sort_values(by='Time')

data = data[data['Month'] > 3]

# Binary classification: Map 'class_vis' to binary classes
data['class_vis'] = data['class_vis'].map({'clear': 0, 'fog': 1})

# Create lag features
lag_features = ['T2','U','V','RH2','P_sfc']  # Replace with your actual feature names
for feature in lag_features:
    for lag in range(1, 2):  # Creating lag features for the past 2 time steps
        data[f'{feature}_lag_{lag}'] = data[feature].shift(lag)

# Drop rows with missing values created by lag features
data = data.dropna()

# Split data into features and target
X = data.drop(['Time', 'class_vis', 'Vis','T2-TD2','TD2'], axis=1)  # Dropping 'Time' as it is now decomposed into useful features
y = data['class_vis']

# Train-test split based on time
train_size = int(len(data) * 0.9)

X_train1, X_test = X[:train_size], X[train_size:]
y_train1, y_test = y[:train_size], y[train_size:]

# Calculate the split index
split_index = int(len(X_train1) * 0.8)

# Create training and validation sets
X_train, X_val = X_train1[:split_index], X_train1[split_index:]
y_train, y_val = y_train1[:split_index], y_train1[split_index:]

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.fit_transform(X_test)
X_val_scaled = scaler.fit_transform(X_val)

# Apply SMOTE for data balancing
from imblearn.combine import SMOTEENN
from imblearn.over_sampling import BorderlineSMOTE
smote = SMOTEENN()
#X_train_scaled, y_train = smote.fit_resample(X_train_scaled, y_train)
#X_val_scaled, y_val = smote.fit_resample(X_val_scaled, y_val)
# Reshape data for LSTM
X_train_resampled = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_test_resampled = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))
X_val_resampled = X_val_scaled.reshape((X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))

# Define class labels (0 and 1 for binary classification)
class_labels = np.unique(y_train)

# Compute class weights
class_weights = compute_class_weight(class_weight="balanced", classes=class_labels, y=y_train)

# Convert to dictionary format
class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}

from sklearn.utils.class_weight import compute_sample_weight

# Compute sample weights
sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)
print(class_weights_dict)
print(sample_weights)
print(X_val_scaled.shape)
print(X_train_scaled.shape)


# Setting the initial bias
pos = np.sum(y_train == 1)  # Count of positive samples
neg = np.sum(y_train == 0)  # Count of negative samples

#initial_bias = np.log([pos / neg])
initial_bias = 0
print(initial_bias)

# @title Building the Model St John's

from tensorflow.keras.optimizers import RMSprop, Nadam

# Define custom F1 Score metric
class F1Score(tf.keras.metrics.Metric):
    def __init__(self, name="f1_score", **kwargs):
        super(F1Score, self).__init__(name=name, **kwargs)
        self.precision = Precision()
        self.recall = Recall()

    def update_state(self, y_true, y_pred, sample_weight=None):
        self.precision.update_state(y_true, y_pred, sample_weight)
        self.recall.update_state(y_true, y_pred, sample_weight)

    def result(self):
        p = self.precision.result()
        r = self.recall.result()
        return 2 * (p * r) / (p + r + tf.keras.backend.epsilon())

    def reset_state(self):
        self.precision.reset_state()
        self.recall.reset_state()

# List of metrics including custom F1 score
METRICS = [
    keras.metrics.BinaryCrossentropy(name='cross_entropy'),
    keras.metrics.MeanSquaredError(name='Brier_score'),
    keras.metrics.TruePositives(name='tp'),
    keras.metrics.FalsePositives(name='fp'),
    keras.metrics.TrueNegatives(name='tn'),
    keras.metrics.FalseNegatives(name='fn'),
    keras.metrics.BinaryAccuracy(name='accuracy'),
    keras.metrics.Precision(name='precision'),
    keras.metrics.Recall(name='recall'),
    keras.metrics.AUC(name='auc'),
    keras.metrics.AUC(name='prc', curve='PR'),  # Precision-recall curve
    F1Score()
]



# Define LSTM layers
model = Sequential([
    Input(shape=(X_train_resampled.shape[1], X_train_resampled.shape[2])),  # Input shape

    # First LSTM Layer
    Bidirectional(LSTM(32, activation='tanh', return_sequences=True)),  # tanh is default lstm
    BatchNormalization(),
    Dropout(0.2),

    # Second LSTM Layer
    Bidirectional(LSTM(32, activation='tanh', return_sequences=False)),
    BatchNormalization(),
    Dropout(0.2),

    # Fully Connected Layers
    Dense(16, activation=None),  # No activation, since we use LeakyReLU
    LeakyReLU(negative_slope=0.01),
    BatchNormalization(),
    Dropout(0.2),

    Dense(16, activation=None),
    LeakyReLU(negative_slope=0.01),
    BatchNormalization(),

    Dropout(0.2),

    # Output Layer
    Dense(1, activation='sigmoid', bias_initializer=Constant(initial_bias))  # Binary classification
])

# Set the initial learning rate
learning_rate = 0.0001
optimizer = Adam(learning_rate=learning_rate)
#optimizer = RMSprop(learning_rate=learning_rate, rho=0.9, epsilon=1e-06)
#optimizer = Nadam(learning_rate=learning_rate)
# Compile the model
model.compile(optimizer= optimizer, loss='binary_crossentropy', metrics= METRICS) # [Precision(), Recall(),F1Score() ]

# Train the model
early_stopping = EarlyStopping(monitor='val_f1_score', patience=20, restore_best_weights=True, mode='max')
reduce_lr = ReduceLROnPlateau(monitor='val_f1_score', factor=0.5, patience=5, min_lr=1e-7, mode='max')

history = model.fit(X_train_resampled, y_train, epochs=200, batch_size= 128, validation_data= (X_val_resampled, y_val), callbacks=[early_stopping, reduce_lr], class_weight= class_weights_dict)  #, sample_weight = sample_weights

# Save the model
#model.save('lstm_onebinary.keras')

# Plotting the results
plt.figure(figsize=(14, 6))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['f1_score'], label='F1Score')
plt.plot(history.history['val_f1_score'], label='val_F1Score')
plt.xlabel('Epoch')
plt.ylabel('F1Score')
plt.legend()
plt.title('(a) Model F1-Score over Epochs')

# Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label='val_loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('(b) Model Loss over Epochs')
plt.suptitle("LSTM Model Performance for St John's", fontsize=16)
plt.show()

# amount of parameters
model.summary()

# @title Visualizing the Performance
# Evaluate the model
y_pred = model.predict(X_test_resampled)
y_pred = y_pred.reshape(-1)
y_pred_classes = (y_pred > 0.82).astype("int32")

# Classification report
print("Classification Report:")
print(classification_report(y_test, y_pred_classes))

# Confusion matrix
cm = confusion_matrix(y_test, y_pred_classes)

# Plot confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['clear', 'fog'], yticklabels=['clear', 'fog'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for St Johns')
plt.show()

"""## Yarmouth"""

# @title Building the model for Yarmouth
# Define LSTM layers
model = Sequential([
    Input(shape=(X_train_resampled.shape[1], X_train_resampled.shape[2])),  # Input shape

    # First LSTM Layer
    Bidirectional(LSTM(16, activation='tanh', return_sequences=True)),  # tanh is default lstm
    BatchNormalization(),
    Dropout(0.2),

    # Second LSTM Layer
    Bidirectional(LSTM(16, activation='tanh', return_sequences=False)),
    BatchNormalization(),
    Dropout(0.2),

    # Fully Connected Layers
    Dense(8, activation=None),  # No activation, since we use LeakyReLU
    LeakyReLU(negative_slope=0.01),
    BatchNormalization(),
    Dropout(0.2),

    Dense(8, activation=None),
    LeakyReLU(negative_slope=0.01),
    BatchNormalization(),

    Dropout(0.2),

    # Output Layer
    Dense(1, activation='sigmoid', bias_initializer=Constant(initial_bias))  # Binary classification
])

# Set the initial learning rate
learning_rate = 0.0001
optimizer = Adam(learning_rate=learning_rate)
#optimizer = RMSprop(learning_rate=learning_rate, rho=0.9, epsilon=1e-06)
#optimizer = Nadam(learning_rate=learning_rate)
# Compile the model
model.compile(optimizer= optimizer, loss='binary_crossentropy', metrics= METRICS) # [Precision(), Recall(),F1Score() ]

# Train the model
early_stopping = EarlyStopping(monitor='val_f1_score', patience=20, restore_best_weights=True, mode='max')
reduce_lr = ReduceLROnPlateau(monitor='val_f1_score', factor=0.5, patience=5, min_lr=1e-7, mode='max')

history = model.fit(X_train_resampled, y_train, epochs=200, batch_size= 128, validation_data= (X_val_resampled, y_val), callbacks=[early_stopping, reduce_lr], class_weight= class_weights_dict)  #, sample_weight = sample_weights

# Save the model
#model.save('lstm_onebinary.keras')

# Plotting the results
plt.figure(figsize=(14, 6))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['f1_score'], label='F1Score')
plt.plot(history.history['val_f1_score'], label='val_F1Score')
plt.xlabel('Epoch')
plt.ylabel('F1Score')
plt.legend()
plt.title('(a) Model F1-Score over Epochs')

# Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label='val_loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('(b) Model Loss over Epochs')
plt.suptitle('LSTM Model Performance for Yarmouth', fontsize=16)
plt.show()

# amount of parameters
model.summary()

# @title Visualizing the Performance for Yarmouth
# Evaluate the model
y_pred = model.predict(X_test_resampled)
y_pred = y_pred.reshape(-1)
y_pred_classes = (y_pred > 0.75).astype("int32")

# Classification report
print("Classification Report:")
print(classification_report(y_test, y_pred_classes))

# Confusion matrix
cm = confusion_matrix(y_test, y_pred_classes)

# Plot confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['clear', 'fog'], yticklabels=['clear', 'fog'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for Yarmouth(LSTM)')
plt.show()

# @title New data (St John's)
# Define the path and pattern for the CSV files
path = 'testStjohn2024_1KM.csv'

# Compute the Dask DataFrame to get a Pandas DataFrame
combined_df = pd.read_csv(path)
# Remove rows where the first column is blank
df_cleaned1 = combined_df[combined_df['Vis'].notnull()]

# Save the cleaned DataFrame back to a CSV file
df_cleaned1.to_csv('cleaned1_file.csv', index=False)

# @title Data Preparation
# Convert 'Time' column to datetime with the correct format
data1 = df_cleaned1.copy()
data1['Time'] = pd.to_datetime(data1['Time'], format='%Y-%m-%d_%H:%M:%S', errors='coerce')

# Extract time-based features
data1['Hour'] = data1['Time'].dt.hour
data1['Day'] = data1['Time'].dt.day
data1['Month'] = data1['Time'].dt.month
data1['DayOfWeek'] = data1['Time'].dt.dayofweek

# Sort the data by time
data1 = data1.sort_values(by='Time')

data1 = data1[data1['Month'] > 3]

# Binary classification: Map 'class_vis' to binary classes
data1['class_vis'] = data1['class_vis'].map({'clear': 0, 'fog': 1})

# Create lag features
lag_features = ['T2','U','V','RH2','P_sfc']  # Replace with your actual feature names
for feature in lag_features:
    for lag in range(1, 2):  # Creating lag features for the past 2 time steps
        data1[f'{feature}_lag_{lag}'] = data1[feature].shift(lag)

# Drop rows with missing values created by lag features
data1 = data1.dropna()

# Split data into features and target
X1 = data1.drop(['Time', 'class_vis', 'Vis'], axis=1)  # Dropping 'Time' as it is now decomposed into useful features
y_new = data1['class_vis']

# Standardize the features
X_new_scaled = scaler.fit_transform(X1)

# Reshape data for LSTM
X_new_resampled = X_new_scaled.reshape((X_new_scaled.shape[0], 1, X_new_scaled.shape[1]))

# Evaluate the model
y_new_pred = model.predict(X_new_resampled)
y_new_pred = y_new_pred.reshape(-1)
y_new_pred_classes = (y_new_pred > 0.85).astype("int32")

# Classification report
print("Classification Report:")
print(classification_report(y_new, y_new_pred_classes))

# Confusion matrix
cm = confusion_matrix(y_new, y_new_pred_classes)

# Plot confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['clear', 'fog'], yticklabels=['clear', 'fog'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for St Johns 2024(LSTM)')
plt.show()

# @title New data (Yarmouth)
# Define the path and pattern for the CSV files
path = 'testYarmouth2024_1KM.csv'

# Compute the Dask DataFrame to get a Pandas DataFrame
combined_df = pd.read_csv(path)
# Remove rows where the first column is blank
df_cleaned1 = combined_df[combined_df['Vis'].notnull()]

# Save the cleaned DataFrame back to a CSV file
df_cleaned1.to_csv('cleaned1_file.csv', index=False)

# @title Data Preparation
# Convert 'Time' column to datetime with the correct format
data1 = df_cleaned1.copy()
data1['Time'] = pd.to_datetime(data1['Time'], format='%Y-%m-%d_%H:%M:%S', errors='coerce')

# Extract time-based features
data1['Hour'] = data1['Time'].dt.hour
data1['Day'] = data1['Time'].dt.day
data1['Month'] = data1['Time'].dt.month
data1['DayOfWeek'] = data1['Time'].dt.dayofweek

# Sort the data by time
data1 = data1.sort_values(by='Time')

data1 = data1[data1['Month'] > 3]

# Binary classification: Map 'class_vis' to binary classes
data1['class_vis'] = data1['class_vis'].map({'clear': 0, 'fog': 1})

# Create lag features
lag_features = ['T2','U','V','RH2','P_sfc']  # Replace with your actual feature names
for feature in lag_features:
    for lag in range(1, 2):  # Creating lag features for the past 2 time steps
        data1[f'{feature}_lag_{lag}'] = data1[feature].shift(lag)

# Drop rows with missing values created by lag features
data1 = data1.dropna()

# Split data into features and target
X1 = data1.drop(['Time', 'class_vis', 'Vis'], axis=1)  # Dropping 'Time' as it is now decomposed into useful features
y_new = data1['class_vis']

# Standardize the features
X_new_scaled = scaler.fit_transform(X1)

# Reshape data for LSTM
X_new_resampled = X_new_scaled.reshape((X_new_scaled.shape[0], 1, X_new_scaled.shape[1]))

# Evaluate the model
y_new_pred = model.predict(X_new_resampled)
y_new_pred = y_new_pred.reshape(-1)
y_new_pred_classes = (y_new_pred > 0.75).astype("int32")

# Classification report
print("Classification Report:")
print(classification_report(y_new, y_new_pred_classes))

# Confusion matrix
cm = confusion_matrix(y_new, y_new_pred_classes)

# Plot confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['clear', 'fog'], yticklabels=['clear', 'fog'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for Yarmouth 2024(LSTM)')
plt.show()

