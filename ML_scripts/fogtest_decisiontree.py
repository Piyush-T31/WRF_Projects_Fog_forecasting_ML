# -*- coding: utf-8 -*-
"""FogTest_Decisiontree

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o6rT8TsVhDmMwbLriSkDw-HHhczoRzIb

# Random Forest Method:
"""

# Commented out IPython magic to ensure Python compatibility.
#@title Import relevant modules
import numpy as np
import pandas as pd
from collections import Counter
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
from sklearn.utils import resample
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import BorderlineSMOTE, SMOTE
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, ExtraTreesRegressor
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,  make_scorer,confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
import glob
import dask.dataframe as dd
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
import warnings, gc, joblib
warnings.filterwarnings('ignore')
from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score, KFold, GridSearchCV, RepeatedKFold, TimeSeriesSplit
from sklearn.linear_model import LogisticRegression, SGDClassifier, PassiveAggressiveClassifier, RidgeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC, LinearSVC, NuSVC
from sklearn.naive_bayes import GaussianNB, BernoulliNB
from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, VotingClassifier, ExtraTreesClassifier, StackingClassifier
from sklearn.neural_network import MLPClassifier
from lightgbm import LGBMClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score
from scipy.stats import probplot, randint
from imblearn.over_sampling import BorderlineSMOTE
from collections import Counter
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectKBest, SelectPercentile, SelectFromModel, f_classif
from subprocess import call
from IPython.display import Image
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint

# The following lines adjust the granularity of reporting.
pd.options.display.max_rows = 10
pd.options.display.float_format = "{:.1f}".format

print("Imported modules.")

# @title Cleaning the Data
# Load the CSV file into a DataFrame
# Define the path and pattern for the CSV files
path = 'Era5_Stjohn.csv'

# Read all CSV files into a Dask DataFrame
dask_df = dd.read_csv(path)

# Compute the Dask DataFrame to get a Pandas DataFrame
combined_df = dask_df.compute()
# Remove rows where the first column is blank
df_cleaned = combined_df[combined_df['Vis'].notnull()]

# Save the cleaned DataFrame back to a CSV file
df_cleaned.to_csv('cleaned_file1.csv', index=False)

# Define the path and pattern for the CSV files
path2 = 'Era5_Yarmouth.csv'

# Read all CSV files into a Dask DataFrame
dask_df2 = dd.read_csv(path2)

# Compute the Dask DataFrame to get a Pandas DataFrame
combined_df2 = dask_df2.compute()
# Remove rows where the first column is blank
df_cleaned2 = combined_df2[combined_df2['Vis'].notnull()]

# Save the cleaned DataFrame back to a CSV file
df_cleaned2.to_csv('cleaned_file2.csv', index=False)
# Display the first 10 entries
#print(df.head(10))

# @title Data Preparation
# load the data
data = pd.read_csv('cleaned_file1.csv')
data1 = pd.read_csv('cleaned_file2.csv')
# Parse the Time column
data['Time'] = pd.to_datetime(data['Time'], format='%Y-%m-%d_%H:%M:%S')

# Extract time-based features
data['Hour'] = data['Time'].dt.hour
data['Day'] = data['Time'].dt.day
data['Month'] = data['Time'].dt.month
data['DayOfWeek'] = data['Time'].dt.dayofweek

# Encode the target variable
label_encoder = LabelEncoder()
data['class_vis'] = label_encoder.fit_transform(data['class_vis'])

# Split data into features and target
X = data.drop(['Time', 'class_vis', 'Vis','TD2','T2-TD2'], axis=1)  # Dropping 'Time' as it is now decomposed into useful features
y = data['class_vis']
X2 = data1.drop(['Time', 'class_vis', 'Vis','TD2','T2-TD2'], axis=1)  # Dropping 'Time' as it is now decomposed into useful features
# Feature scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X2_scaled = scaler.fit_transform(X2)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)

# Apply SMOTE for data balancing
from imblearn.combine import SMOTEENN
from imblearn.over_sampling import BorderlineSMOTE
#smote = BorderlineSMOTE()
smote = SMOTEENN()
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

import matplotlib.pyplot as plt
import seaborn as sns
from imblearn.over_sampling import SMOTE

#two features are chosen, 'RH2' and 'T2' and a binary target 'class'
X = data[['RH2', 'T2']]  # Choose 2 features for the 2D scatter plot
y = data['class_vis']  # Binary target

# Apply SMOTE
smote = SMOTEENN()
X_resampled, y_resampled = smote.fit_resample(X, y)

# Combine the resampled data for easier plotting
resampled_data = pd.DataFrame(X_resampled, columns=['RH2', 'T2'])
resampled_data['class'] = y_resampled

# Plot 1: Before SMOTE
plt.figure(figsize=(18, 10))

plt.subplot(1, 2, 1)  # 1 row, 2 columns, first plot

# Access the columns using their names to avoid the error
plt.scatter(X['RH2'], X['T2'], c=y, cmap="viridis", marker="o", alpha=0.7)

plt.title("Before SMOTE")
plt.xlabel("Relative Humidity at 2m/ %")
plt.ylabel("Temperature at 2m/K")
plt.colorbar(label='Class: clear=0, fog=1')

# Plot 2: After SMOTE
plt.subplot(1, 2, 2)  # 1 row, 2 columns, second plot

# Access the columns using their names for X_resampled as well
plt.scatter(X_resampled['RH2'], X_resampled['T2'], c=y_resampled, cmap="viridis", marker="x", alpha=0.7)

plt.title("After SMOTE")
plt.xlabel("Relative Humidity at 2m/ %")
plt.ylabel("Temperature at 2m/K")
plt.colorbar(label='Class: clear=0, fog=1')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import pandas as pd
from imblearn.combine import SMOTEENN

# Assume 'data' has 'RH2', 'T2', and 'class_vis'
X = data[['RH2', 'T2']]
y = data['class_vis']

# Apply SMOTEENN
smote = SMOTEENN()
X_resampled, y_resampled = smote.fit_resample(X, y)

# Create DataFrames
original_data = X.copy()
original_data['class'] = y

resampled_data = pd.DataFrame(X_resampled, columns=['RH2', 'T2'])
resampled_data['class'] = y_resampled

# Set up figure
plt.figure(figsize=(18, 8))

# Plot 1: Before SMOTEENN
plt.subplot(1, 2, 1)
for label, color, alpha_val in zip([0, 1], ['skyblue', 'orange'], [0.3, 0.8]):
    subset = original_data[original_data['class'] == label]
    plt.scatter(subset['RH2'], subset['T2'],
                label=f"{'Clear' if label == 0 else 'Fog'}",
                alpha=alpha_val, color=color)
plt.title("Before SMOTEENN")
plt.xlabel("Relative Humidity at 2m (%)")
plt.ylabel("Temperature at 2m (K)")
plt.legend()

# Plot 2: After SMOTEENN
plt.subplot(1, 2, 2)
for label, color, alpha_val in zip([0, 1], ['skyblue', 'orange'], [0.3, 0.8]):
    subset = resampled_data[resampled_data['class'] == label]
    plt.scatter(subset['RH2'], subset['T2'],
                label=f"{'Clear' if label == 0 else 'Fog'}",
                alpha=alpha_val, color=color, marker='x')
plt.title("After SMOTEENN")
plt.xlabel("Relative Humidity at 2m (%)")
plt.ylabel("Temperature at 2m (K)")
plt.legend()

plt.tight_layout()
plt.show()

# Load the data
data = pd.read_csv('cleaned_file1.csv')
data1 = pd.read_csv('cleaned_file2.csv')

# Assign data for clarity
X = data.drop(['Time', 'class_vis', 'Vis','TD2','T2-TD2'], axis=1)  # Dropping 'Time' as it is now decomposed into useful features
X2 = data1.drop(['Time', 'class_vis', 'Vis','TD2','T2-TD2'], axis=1)  # Dropping 'Time' as it is now decomposed into useful features

# Rescale 'P_sfc' by dividing by 1000 for both datasets
X['P_sfc'] = X['P_sfc'] / 1000
X2['P_sfc'] = X2['P_sfc'] / 1000

# Create a dictionary for units
unit_dict = {
    'T2': 'K',   # Temperature
    'RH2': '%',   # Relative Humidity
    'U': 'm/s',   # Wind Speed
    'V': 'm/s',   # Wind Speed (or direction, specify if needed)
    'P_sfc': 'kPa' # Surface Pressure (after rescaling)
}

# Rename the columns to include units
X.rename(columns={col: f"{col} ({unit_dict[col]})" for col in X.columns}, inplace=True)
X2.rename(columns={col: f"{col} ({unit_dict[col]})" for col in X2.columns}, inplace=True)

# List of selected columns (now including units in the names)
selected_cols = [f"{col} ({unit_dict[col]})" for col in ['T2', 'RH2', 'U', 'V', 'P_sfc']]

# Plot distributions
rows, cols = (len(selected_cols) + 1) // 2, 2
plt.figure(figsize=(3 * cols, 3 * rows))

for i, col in enumerate(selected_cols, 1):
    plt.subplot(rows, cols, i)
    # Plot for both datasets
    sns.histplot(X[col], color='blue', label='St Johns', kde=True, stat="density", common_norm=False)
    sns.histplot(X2[col], color='orange', label='Yarmouth', kde=True, stat="density", common_norm=False)

    # Update the title and labels with units
    plt.title(f"{col}")
    plt.xlabel(f"{col}")
    plt.ylabel('Density')
    plt.legend()

plt.suptitle('Distribution Plots for St John\'s and Yarmouth', y=1.02)
plt.tight_layout()
plt.show()

#calculating skewness and kurtosis for Yarmouth and St John's
import pandas as pd

# Load the cleaned datasets
df1 = pd.read_csv('cleaned_file1.csv')
df2 = pd.read_csv('cleaned_file2.csv')

selected_cols = ['T2', 'RH2', 'U', 'V', 'P_sfc']

# Function to calculate skewness and kurtosis
def calculate_stats(df, col_name):
    skewness = df[col_name].skew()
    kurtosis = df[col_name].kurtosis()
    return skewness, kurtosis

# Print skewness and kurtosis for each selected column in both datasets
for col in selected_cols:
    skew1, kurt1 = calculate_stats(df1, col)
    skew2, kurt2 = calculate_stats(df2, col)
    print(f"Column: {col}")
    print(f"  Dataset 1 (cleaned_file1.csv): Skewness = {skew1:.2f}, Kurtosis = {kurt1:.2f}")
    print(f"  Dataset 2 (cleaned_file2.csv): Skewness = {skew2:.4f}, Kurtosis = {kurt2:.4f}")

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# List of specific columns to plot (common columns in both X and X2)
selected_cols = ['T2', 'RH2', 'U', 'V', 'P_sfc']  # Columns of relevant features

# --- Plot: Boxplots for Both Datasets in Rows ---
plt.figure(figsize=(6, 3 * ((len(selected_cols) + 1) // 2)))  # Adjust figure size (width for each column, fixed height)

for i, col in enumerate(selected_cols, 1):
    plt.subplot((len(selected_cols) + 1) // 2, 2, i)

    # Use the column index instead of the column name to access data in the NumPy array
    col_index = X.columns.get_loc(col)  # Get the column index for 'col' in X

    # Get the minimum number of rows between the two arrays
    min_rows = min(X_scaled.shape[0], X2_scaled.shape[0])

    # Create a DataFrame to merge the data from both X and X2 for each column,
    # using only the first 'min_rows' rows of each array
    data_to_plot = pd.DataFrame({
        'St John\'s': X_scaled[:min_rows, col_index],
        'Yarmouth': X2_scaled[:min_rows, col_index]
    })

    # Melt the data for seaborn's boxplot (long-form DataFrame)
    data_melted = data_to_plot.melt(var_name='Locations', value_name=col)

    # Plot the boxplot, with data from both datasets
    sns.boxplot(x='Locations', y=col, data=data_melted)

    plt.title(f"{col}")
    plt.xlabel('Locations')
    plt.ylabel(col)

plt.tight_layout()
plt.suptitle('Boxplots for St John\'s and Yarmouth', y=1.02)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the data
data = pd.read_csv('cleaned_file1.csv')
data1 = pd.read_csv('cleaned_file2.csv')

# Assign data for clarity
X = data.drop(['Time', 'class_vis', 'Vis', 'TD2', 'T2-TD2'], axis=1)
X2 = data1.drop(['Time', 'class_vis', 'Vis', 'TD2', 'T2-TD2'], axis=1)

# Rescale 'P_sfc' by dividing by 1000 for both datasets
X['P_sfc'] = X['P_sfc'] / 1000
X2['P_sfc'] = X2['P_sfc'] / 1000

# Create a dictionary for units
unit_dict = {
    'T2': 'K',
    'RH2': '%',
    'U': 'm/s',
    'V': 'm/s',
    'P_sfc': 'kPa'
}

# Rename columns to include units
X.rename(columns={col: f"{col} ({unit_dict[col]})" for col in X.columns}, inplace=True)
X2.rename(columns={col: f"{col} ({unit_dict[col]})" for col in X2.columns}, inplace=True)

# List of selected columns
selected_cols = [f"{col} ({unit_dict[col]})" for col in ['T2', 'RH2', 'U', 'V', 'P_sfc']]

# Convert to long format for Seaborn
X_long = X[selected_cols].melt(var_name="Variable", value_name="Value")
X_long["Location"] = "St. John's"

X2_long = X2[selected_cols].melt(var_name="Variable", value_name="Value")
X2_long["Location"] = "Yarmouth"

# Combine both datasets
df_long = pd.concat([X_long, X2_long])

# Plot boxplots
rows, cols = (len(selected_cols) + 1) // 2, 2
plt.figure(figsize=(3 * cols, 3 * rows))

for i, col in enumerate(selected_cols, 1):
    plt.subplot(rows, cols, i)

    # Subset data for this variable
    subset = df_long[df_long["Variable"] == col]

    # Boxplot with grouped locations
    sns.boxplot(x="Location", y="Value", data=subset, palette=['blue', 'orange'])

    # Title and labels
    plt.title(col)
    plt.ylabel(col)

plt.suptitle('Boxplots for St. John\'s and Yarmouth', y=1.02)
plt.tight_layout()
plt.show()

# @title Preprocessing the Data
y.value_counts()
print(y.value_counts())
y_train_res.value_counts()
print(y_train_res.value_counts())

import pandas as pd
import matplotlib.pyplot as plt

# Load your CSV file
df = pd.read_csv("cleaned_file2.csv")

# Fix the datetime column format
df['Time'] = pd.to_datetime(df['Time'], format="%Y-%m-%d_%H:%M:%S")

# Filter data from April to August
df_apr_aug = df[df['Time'].dt.month.isin([4, 5, 6, 7, 8])]

# ---- Plot 1: Fog vs Clear ----
fog_clear_counts = df_apr_aug['class_vis'].value_counts()
fog_clear_counts = fog_clear_counts[['fog', 'clear']]  # Consistent order

# ---- Plot 2: Monthly fog occurrences ----
fog_df = df_apr_aug[df_apr_aug['class_vis'] == 'fog'].copy()
fog_df['Month'] = fog_df['Time'].dt.month_name()
month_order = ['April', 'May', 'June', 'July', 'August']
monthly_fog_counts = fog_df['Month'].value_counts().reindex(month_order)

# ---- Plotting ----
fig, axes = plt.subplots(2, 1, figsize=(8, 10))

# Plot 1: Fog vs Clear (horizontal bar)
colors = ['skyblue', 'lightcoral']
axes[0].barh(fog_clear_counts.index, fog_clear_counts.values, color=colors)
axes[0].set_title("Fog vs Clear in Yarmouth (April to August, 2012â€“2023)")
axes[0].set_xlabel("Total Occurrences")

# Plot 2: Monthly Fog Occurrences
axes[1].bar(monthly_fog_counts.index, monthly_fog_counts.values, color='skyblue')
axes[1].set_title("Monthly Fog Occurrences For Yarmouth (2012â€“2023)")
axes[1].set_xlabel("Month")
axes[1].set_ylim(0, 2300)
axes[1].set_ylabel("Fog Occurrences")

plt.tight_layout()
plt.show()

# V and RH2 joint pdf scatter plots

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the datasets
df1 = pd.read_csv('cleaned_file1.csv')
df2 = pd.read_csv('cleaned_file2.csv')

# Select relevant columns
selected_cols = ['T2', 'RH2', 'U', 'V', 'P_sfc', 'Vis']
data = df2[selected_cols]

# Calculate wind speed and add it as a new column
data['Wind_Speed'] = (data['U']**2 + data['V']**2)**0.5

# Extract 'RH2' and 'V' for the joint PDF
x = data['RH2']
y = data['V']
# Create a joint PDF plot for RH2 and V
plt.figure(figsize=(8, 6))
# Assign the result of sns.kdeplot to a variable
contour_plot = sns.kdeplot(x=x, y=y, cmap="viridis", fill=True, levels=50, thresh=0.01, cbar=True)  #    levels=np.linspace(0, 0.0041, 50)

# Customize the plot
plt.title('Joint PDF of RH2 and V for Yarmouth', fontsize=14)
plt.xlabel('RH2 (%)', fontsize=12)
plt.ylabel('V (m/s)', fontsize=12)
plt.grid(True)
plt.show()

# looking at the fog days
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('cleaned_file2.csv')

# Convert 'Time' column to datetime and extract the hour of the day
df['Time'] = pd.to_datetime(df['Time'], format='%Y-%m-%d_%H:%M:%S')
df['Hour'] = df['Time'].dt.hour

# Filter fog occurrences
fog_data = df[df['class_vis'] == 'fog']

# Calculate wind speed (based on U and V components)
fog_wind_speed = (fog_data['U']**2 + fog_data['V']**2)**0.5
U = fog_data['U']
V = fog_data['V']
# Plot: Fog occurrence by hour (frequency)
plt.figure(figsize=(12, 6))
fog_count_by_hour = fog_data['Hour'].value_counts().sort_index()
plt.plot(fog_count_by_hour.index, fog_count_by_hour.values, marker='o', color='blue', label='Fog Frequency')

plt.title('Fog Occurrence by Hour of Day (Yarmouth)', fontsize=14)
plt.xlabel('Hour of Day/ UTC', fontsize=12)
plt.ylabel('Fog Count', fontsize=12)
plt.ylim(75, 500)
plt.xticks(range(0, 24))
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.legend()
plt.tight_layout()
plt.show()

# Plot: Fog wind speed by hour (scatter plot)
plt.figure(figsize=(12, 6))
plt.scatter(fog_data['Hour'], U, alpha=0.6, color='green', edgecolor='k')

plt.title('Fog Occurrence: U vs. Time of Day (Yarmouth)', fontsize=14)
plt.xlabel('Hour of Day/ UTC', fontsize=12)
plt.ylabel('U (m/s)', fontsize=12)
plt.xticks(range(0, 24))
plt.grid(axis='both', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

# polar plot wind speed and wind direction
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import gaussian_kde

# Load the dataset
file_path = "cleaned_file2.csv"
cleaned_data = pd.read_csv(file_path)

# Filter for foggy conditions
fog_data = cleaned_data[cleaned_data['class_vis'] == 'fog']

# Compute wind speed and meteorological wind direction
fog_data['wind_speed'] = np.sqrt(fog_data['U']**2 + fog_data['V']**2)
fog_data['wind_direction'] = (np.arctan2(fog_data['V'], fog_data['U']) * 180 / np.pi) % 360
fog_data['wind_direction_met'] = (360 - fog_data['wind_direction']) % 360  # Meteorological convention

# Convert wind direction to radians for plotting
wind_speed = fog_data['wind_speed']
wind_direction_met_radians = np.deg2rad(fog_data['wind_direction_met'])

# Perform 2D KDE for meteorological wind direction and wind speed
values_met = np.vstack([wind_direction_met_radians, wind_speed])
kde_met = gaussian_kde(values_met)

# Create a grid for the PDF
theta_met, r = np.meshgrid(
    np.linspace(0, 2 * np.pi, 100),  # Full circle
    np.linspace(0, wind_speed.max(), 100)  # Wind speed range
)
grid_coords_met = np.vstack([theta_met.ravel(), r.ravel()])

# Evaluate the KDE on the grid
pdf_met = kde_met(grid_coords_met).reshape(theta_met.shape)

# Plot the joint PDF using meteorological conventions
fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111, polar=True)
c = ax.contourf(theta_met, r, pdf_met, levels = 50, cmap='viridis') #levels=np.linspace(0, 0.09, 50)
ax.tick_params(colors='white')
fig.colorbar(c, ax=ax, label='PDF Value')

# Set the angular labels for meteorological conventions (clockwise, 0Â° = North)
ax.set_theta_direction(-1)  # Clockwise
ax.set_theta_offset(np.pi / 2)  # 0Â° at the top (North)
ax.set_ylim(0, 14)

ax.set_title("Joint PDF of Wind Speed and Direction (Yarmouth)", pad=30)
plt.show()

!pip install windrose

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from windrose import WindroseAxes

# Load data
file_path = "cleaned_file1.csv"
cleaned_data = pd.read_csv(file_path)

# Filter for fog
fog_data = cleaned_data[cleaned_data['class_vis'] == 'fog'].copy()

# Compute wind speed and direction (meteorological)
fog_data['wind_speed'] = np.sqrt(fog_data['U']**2 + fog_data['V']**2)
fog_data['wind_direction'] = (np.arctan2(fog_data['V'], fog_data['U']) * 180 / np.pi) % 360
fog_data['wind_direction_met'] = (360 - fog_data['wind_direction']) % 360

# Create the windrose plot
fig = plt.figure(figsize=(8, 6))
ax = WindroseAxes.from_ax(fig=fig)
ax.bar(
    fog_data['wind_direction_met'],
    fog_data['wind_speed'],
    bins=np.arange(0, 14, 2),  # Define wind speed bins
    opening=0.8,
    edgecolor='white',
    cmap=plt.get_cmap('viridis'),
    normed=True                # Optional: If normalize=True gives error, use normed
)

# Final touches
ax.set_title("Wind Rose during Fog Conditions (St John's)", y= 1.05)
# Set radial (wind speed) limit
ax.set_ylim(0, 23)  # For example, limit to 25%
# Optional: Set specific tick labels on the radial axis (percent)
ax.set_yticks([5, 10, 15, 20, 25])
ax.set_yticklabels(['5%', '10%', '15%', '20%', '25%'])  # if using normalized=True
ax.set_legend(title="Wind Speed (m/s)")
plt.show()

# @title Find the best parameters
# Define the parameter distribution for RandomizedSearchCV
param_dist = {
    'n_estimators': randint(100, 300),
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': randint(2, 11),
    'min_samples_leaf': randint(1, 5),
    'bootstrap': [True, False]
}

# Initialize the Random Forest classifier
rf_classifier = RandomForestClassifier(random_state=42)

# Initialize RandomizedSearchCV
random_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_dist, n_iter=30, cv=2, n_jobs=-1, random_state=42, verbose=2)

# Fit the model
random_search.fit(X_train_res, y_train_res)

# Print the best parameters
print("Best parameters found: ", random_search.best_params_)

# Get the best estimator
best_rf_classifier = random_search.best_estimator_

# Predictions
y_pred = best_rf_classifier.predict(X_test)

# Evaluation
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print(f"Random Forest Accuracy: {accuracy:.4f}")
print(f"Random Forest Precision: {precision:.4f}")
print(f"Random Forest Recall: {recall:.4f}")
print(f"Random Forest F1-Score: {f1:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Feature Importance
importances = best_rf_classifier.feature_importances_
feature_names = X.columns
feature_importance_df = pd.DataFrame(importances, index=feature_names, columns=['Importance']).sort_values('Importance', ascending=False)
print("\nFeature Importance:\n", feature_importance_df)

# Visualization of Feature Importance
plt.figure(figsize=(10, 6))
sns.barplot(x=feature_importance_df['Importance'], y=feature_importance_df.index)
plt.title('Feature Importance')
plt.show()

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# @title Building the Model
# Initialize and train Random Forest
#rf_classifier = RandomForestClassifier(random_state=42, n_estimators=200)
rf_classifier = RandomForestClassifier(
    n_estimators=175,  # Number of trees
    max_depth=None,      # Maximum depth of each tree
    min_samples_split=2,  # Minimum number of samples required to split an internal node
    min_samples_leaf=1,   # Minimum number of samples required to be at a leaf node
    bootstrap= False,       # Whether bootstrap samples are used when building trees
    random_state=42
)
# Fit the model
rf_classifier.fit(X_train_res, y_train_res)

# Predictions
y_pred = rf_classifier.predict(X_test)

# Evaluation
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print(f"Random Forest Accuracy: {accuracy:.4f}")
print(f"Random Forest Precision: {precision:.4f}")
print(f"Random Forest Recall: {recall:.4f}")
print(f"Random Forest F1-Score: {f1:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Cross-Validation
cv_scores = cross_val_score(rf_classifier, X_scaled, y, cv=5)
print(f"Cross-Validation Scores: {cv_scores}")
print(f"Mean CV Accuracy: {cv_scores.mean():.4f}")

# Feature Importance
importances = rf_classifier.feature_importances_
feature_names = X.columns
feature_importance_df = pd.DataFrame(importances, index=feature_names, columns=['Importance']).sort_values('Importance', ascending=False)
print("\nFeature Importance:\n", feature_importance_df)

# Visualization of Feature Importance
plt.figure(figsize=(10, 6))
sns.barplot(x=feature_importance_df['Importance'], y=feature_importance_df.index)
plt.title('Feature Importance')
plt.show()

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

"""# Binary Classification

## One location
"""

# @title Cleaning the Data
# Load the CSV file into a DataFrame
# Define the path and pattern for the CSV files
path = 'Era5_Yarmouth_1KM.csv'

# Compute the Dask DataFrame to get a Pandas DataFrame
combined_df = pd.read_csv(path)
# Remove rows where the first column is blank
df_cleaned = combined_df[combined_df['Vis'].notnull()]

# Save the cleaned DataFrame back to a CSV file
df_cleaned.to_csv('cleaned_file.csv', index=False)

# @title Data Preparation
# Convert 'Time' column to datetime with the correct format
data = df_cleaned.copy()
data['Time'] = pd.to_datetime(data['Time'], format='%Y-%m-%d_%H:%M:%S', errors='coerce')

# Extract time-based features
data['Hour'] = data['Time'].dt.hour
#data['Day'] = data['Time'].dt.day
data['Month'] = data['Time'].dt.month
#data['sin_hour'] = np.sin(2 * np.pi * data['Hour'] / 24)
#df['cos_hour'] = np.cos(2 * np.pi * data['hour'] / 24)
# Sort the data by time
data = data.sort_values(by='Time')

data = data[data['Month'] > 3]

# Binary classification: Map 'class_vis' to binary classes
data['class_vis'] = data['class_vis'].map({'clear': 0, 'fog': 1})

# Create lag features
lag_features = ['T2','U','V','RH2','P_sfc']  # Replace with your actual feature names
for feature in lag_features:
     for lag in range(1, 2):  # Creating lag features for the past 2 time steps
         data[f'{feature}_lag_{lag}'] = data[feature].shift(lag)

# Define the frequency of your data
lag_hours = 1

# Create a lagged feature for 'class_vis'
#data['class_vis_lag_24hr'] = data['Vis'].shift(lag_hours)

# Drop rows with missing values created by lag features
data = data.dropna()

# Split data into features and target
X = data.drop(['Time', 'class_vis', 'Vis','T2-TD2','TD2'], axis=1)  # Dropping 'Time' as it is now decomposed into useful features
y = data['class_vis']

# Train-test split based on time
train_size = int(len(data) * 0.9)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Apply SMOTE for data balancing
from imblearn.combine import SMOTEENN
from imblearn.over_sampling import BorderlineSMOTE
smote = SMOTEENN()
X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)

# @title Find the parameters for St John

# Define the parameter distribution for RandomizedSearchCV
param_dist = {
    'n_estimators': randint(100, 500),
    'criterion': ['gini', 'log_loss', 'entropy'],
    'max_features': ['sqrt', 'log2'],
    'max_depth': [10, 20, 30, 40, None],
    'min_samples_split': randint(2, 15),
    'min_samples_leaf': randint(1, 5),
    'class_weight': ['balanced', 'balanced_subsample'],
    'bootstrap': [True, False]
}

# Initialize the Extra Trees classifier
et_classifier = ExtraTreesClassifier()

# Initialize RandomizedSearchCV
#random_search = RandomizedSearchCV(estimator=et_classifier, param_distributions=param_dist, scoring= f1_score, n_iter=50, cv=5, n_jobs=-1, verbose=2)

# Define the TimeSeriesSplit cross-validator
tscv = TimeSeriesSplit(n_splits=5)
random_search = RandomizedSearchCV(estimator=et_classifier, param_distributions=param_dist, scoring= make_scorer(f1_score, average='weighted'), n_iter=50, cv=tscv, n_jobs=-1, verbose=2)

# Fit the model
random_search.fit(X_train_res, y_train_res)

# Print the best parameters
print("Best Parameters:", random_search.best_params_)

# @title Find the parameters for St John with lagged Vis

# Define the parameter distribution for RandomizedSearchCV
param_dist = {
    'n_estimators': randint(100, 500),
    'criterion': ['gini', 'log_loss', 'entropy'],
    'max_features': ['sqrt', 'log2'],
    'max_depth': [10, 20, 30, 40, None],
    'min_samples_split': randint(2, 15),
    'min_samples_leaf': randint(1, 5),
    'class_weight': ['balanced', 'balanced_subsample'],
    'bootstrap': [True, False]
}

# Initialize the Extra Trees classifier
et_classifier = ExtraTreesClassifier()

# Initialize RandomizedSearchCV
#random_search = RandomizedSearchCV(estimator=et_classifier, param_distributions=param_dist, scoring= f1_score, n_iter=50, cv=5, n_jobs=-1, verbose=2)

# Define the TimeSeriesSplit cross-validator
tscv = TimeSeriesSplit(n_splits=5)
random_search = RandomizedSearchCV(estimator=et_classifier, param_distributions=param_dist, scoring= make_scorer(f1_score, average='weighted'), n_iter=50, cv=tscv, n_jobs=-1, verbose=2)

# Fit the model
random_search.fit(X_train_res, y_train_res)

# Print the best parameters
print("Best Parameters:", random_search.best_params_)

# @title Post pruning evaluation
# Assuming pre-optimized parameters:
pre_tuned_params = {
    'n_estimators': 270,
    'max_depth': 10,
    'min_samples_split': 2,
    'min_samples_leaf': 1,
    'max_features': 'sqrt',
    'criterion': 'gini',
    'bootstrap': True,
    'class_weight': 'balanced'
}

# Step 1: Find range of `ccp_alpha` values using one ExtraTree
single_tree = ExtraTreesClassifier(**pre_tuned_params).fit(X_train_res, y_train_res)
path = single_tree.estimators_[0].cost_complexity_pruning_path(X_train_res, y_train_res)
ccp_alphas = path.ccp_alphas

# Step 2: Set up a smaller grid search just for `ccp_alpha`
param_grid = {
    'ccp_alpha': ccp_alphas
}

et_clf = ExtraTreesClassifier(**pre_tuned_params)
grid_search = GridSearchCV(estimator=et_clf, param_grid=param_grid, scoring='f1', cv=5)
grid_search.fit(X_train_res, y_train_res)

# Step 3: Evaluate the best model
best_et_model = grid_search.best_estimator_

# Predict and print the classification report
y_pred = best_et_model.predict(X_test_scaled)
print("Best ccp_alpha found:", grid_search.best_params_['ccp_alpha'])
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=['clear', 'fog'], yticklabels=['clear', 'fog'])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix with Pruned ExtraTreesClassifier")
plt.show()

# @title Building the Model (St John's)
# Initialize and train ExtraTreeClassifier
et_classifier = ExtraTreesClassifier(
    n_estimators=105,   # Number of trees 267
    criterion = 'log_loss', # purity gini
    max_depth= None,      # Maximum depth of each tree 10
    max_features= 'sqrt',  # Number of features to consider when looking for the best split sqrt
    min_samples_split=8,  # Minimum number of samples required to split an internal node 2
    min_samples_leaf=4,   # Minimum number of samples required to be at a leaf node 1
    bootstrap= True,    # Whether bootstrap samples are used when building trees True
    class_weight='balanced', # whether to use different weights balanced
    random_state=42,
#    ccp_alpha= 0.00002
)
# Fit the model
et_classifier.fit(X_train_res, y_train_res)

# Predictions
y_train_pred = et_classifier.predict(X_train_scaled)
y_test_prob = et_classifier.predict_proba(X_test_scaled)
y_test_pred = (y_test_prob[:, 1] >= 0.8).astype(int)

# Evaluation on Training Set
train_accuracy = accuracy_score(y_train, y_train_pred)
train_precision = precision_score(y_train, y_train_pred, average='weighted')
train_recall = recall_score(y_train, y_train_pred, average='weighted')
train_f1 = f1_score(y_train, y_train_pred, average='weighted')

print(f"Training Set Evaluation:\nAccuracy: {train_accuracy:.4f}")
print(f"Precision: {train_precision:.4f}")
print(f"Recall: {train_recall:.4f}")
print(f"F1-Score: {train_f1:.4f}")

# Evaluation on Test Set
test_accuracy = accuracy_score(y_test, y_test_pred)
test_precision = precision_score(y_test, y_test_pred, average='weighted')
test_recall = recall_score(y_test, y_test_pred, average='weighted')
test_f1 = f1_score(y_test, y_test_pred, average='weighted')

print(f"Test Set Evaluation:\nAccuracy: {test_accuracy:.4f}")
print(f"Precision: {test_precision:.4f}")
print(f"Recall: {test_recall:.4f}")
print(f"F1-Score: {test_f1:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_test_pred))

# Cross-Validation
cv_scores = cross_val_score(et_classifier, X, y, cv=5)
print(f"Cross-Validation Scores: {cv_scores}")
print(f"Mean CV Accuracy: {cv_scores.mean():.4f}")

# Feature Importance
importances = et_classifier.feature_importances_
feature_names = X.columns
feature_importance_df = pd.DataFrame(importances, index=feature_names, columns=['Importance']).sort_values('Importance', ascending=False)
print("\nFeature Importance:\n", feature_importance_df)

# Visualization of Feature Importance
plt.figure(figsize=(10, 6))
sns.barplot(x=feature_importance_df['Importance'], y=feature_importance_df.index)
plt.title("Feature Importance for St John's")
plt.show()

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['clear', 'fog'], yticklabels=['clear', 'fog'])
plt.title("Confusion Matrix for St John's")
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# @title Visualizing the decision tree
# Step 2: Access one of the trees in the forest (e.g., the first tree)
from sklearn.tree import plot_tree
estimator = et_classifier.estimators_[0]

# Step 3: Visualize the tree
plt.figure(figsize=(20,10))  # Set the figure size for better readability
plot_tree(estimator, feature_names=[f'Feature {i}' for i in range(X_train_res.shape[1])], filled=True, rounded=True)
plt.show()

from sklearn.tree import export_graphviz
import graphviz

# Export the tree to DOT format
dot_data = export_graphviz(estimator, out_file=None,
                           feature_names=X_train.columns,
                           class_names=['fog', 'clear'],
                           filled=True, rounded=True,
                           special_characters=True)

# Render the tree using graphviz
graph = graphviz.Source(dot_data)
graph.render("extra_tree")  # Saves it as a file "extra_tree.png"
graph.view()

# @title Find the parameters for Yarmouth

# Define the parameter distribution for RandomizedSearchCV
param_dist = {
    'n_estimators': randint(100, 500),
    'criterion': ['gini', 'log_loss', 'entropy'],
    'max_features': ['sqrt', 'log2', None],
    'max_depth': [10, 20, 30, 40, None],
    'min_samples_split': randint(2, 15),
    'min_samples_leaf': randint(1, 5),
    'class_weight': ['balanced', 'balanced_subsample'],
    'bootstrap': [True, False]
}

# Initialize the Extra Trees classifier
et_classifier = ExtraTreesClassifier()

# Initialize RandomizedSearchCV
tscv = TimeSeriesSplit(n_splits=5)
random_search = RandomizedSearchCV(estimator=et_classifier, param_distributions=param_dist, scoring= make_scorer(f1_score, average='weighted'), n_iter=50, cv=tscv, n_jobs=-1, verbose=2)

# Fit the model
random_search.fit(X_train_res, y_train_res)

# Print the best parameters
print("Best Parameters:", random_search.best_params_)

"""Best Parameters: {'bootstrap': False, 'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_depth': 40, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 12, 'n_estimators': 476}"""

# @title Building the Model (Yarmouth)
# Initialize and train ExtraTreeClassifier
et_classifier = ExtraTreesClassifier(
    n_estimators= 125,   # Number of trees 375
    criterion = 'entropy', # criteria to split gini
    max_depth= None,      # Maximum depth of each tree 20
    max_features= None,  # Number of features to consider when looking for the best split sqrt
    min_samples_split=4,  # Minimum number of samples required to split an internal node 5
    min_samples_leaf=3,   # Minimum number of samples required to be at a leaf node 2
    bootstrap= True,    # Whether bootstrap samples are used when building trees True
    class_weight='balanced_subsample', # class weight for imbalanced adjusting balanced
    random_state=42,
)
# Fit the model
et_classifier.fit(X_train_res, y_train_res)

# Predictions
y_train_pred = et_classifier.predict(X_train_scaled)
y_test_prob = et_classifier.predict_proba(X_test_scaled)
y_test_pred = (y_test_prob[:, 1] >= 0.8).astype(int)

# Evaluation on Training Set
train_accuracy = accuracy_score(y_train, y_train_pred)
train_precision = precision_score(y_train, y_train_pred, average='weighted')
train_recall = recall_score(y_train, y_train_pred, average='weighted')
train_f1 = f1_score(y_train, y_train_pred, average='weighted')

print(f"Training Set Evaluation:\nAccuracy: {train_accuracy:.4f}")
print(f"Precision: {train_precision:.4f}")
print(f"Recall: {train_recall:.4f}")
print(f"F1-Score: {train_f1:.4f}")

# Evaluation on Test Set
test_accuracy = accuracy_score(y_test, y_test_pred)
test_precision = precision_score(y_test, y_test_pred, average='weighted')
test_recall = recall_score(y_test, y_test_pred, average='weighted')
test_f1 = f1_score(y_test, y_test_pred, average='weighted')

print(f"Test Set Evaluation:\nAccuracy: {test_accuracy:.4f}")
print(f"Precision: {test_precision:.4f}")
print(f"Recall: {test_recall:.4f}")
print(f"F1-Score: {test_f1:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_test_pred))

# Cross-Validation
cv_scores = cross_val_score(et_classifier, X, y, cv=5)
print(f"Cross-Validation Scores: {cv_scores}")
print(f"Mean CV Accuracy: {cv_scores.mean():.4f}")

# Feature Importance
importances = et_classifier.feature_importances_
feature_names = X.columns
feature_importance_df = pd.DataFrame(importances, index=feature_names, columns=['Importance']).sort_values('Importance', ascending=False)
print("\nFeature Importance:\n", feature_importance_df)

# Visualization of Feature Importance
plt.figure(figsize=(10, 6))
sns.barplot(x=feature_importance_df['Importance'], y=feature_importance_df.index)
plt.title('Feature Importance for Yarmouth')
plt.show()

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['clear', 'fog'], yticklabels=['clear', 'fog'])
plt.title('Confusion Matrix for Yarmouth')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# @title Testing with new data (Yarmouth)
# Create a path and read the new data with pd
path1 = 'testYarmouth2024_1KM.csv'

read1 = pd.read_csv(path1)
# Remove any blanks
read1 = read1[read1['Vis'].notnull()]
# Save to a csv file
read1.to_csv('cleaned1_file.csv', index=False)

## Data Preparation

# Convert 'Time' column to datetime with the correct format
data1 = read1.copy()
data1['Time'] = pd.to_datetime(data1['Time'], format='%Y-%m-%d_%H:%M:%S', errors='coerce')

# Extract time-based features
data1['Hour'] = data1['Time'].dt.hour
#data1['Day'] = data1['Time'].dt.day
data1['Month'] = data1['Time'].dt.month

# Sort the data by time
data1 = data1.sort_values(by='Time')

data1 = data1[data1['Month'] > 3]

# Binary classification: Map 'class_vis' to binary classes
data1['class_vis'] = data1['class_vis'].map({'clear': 0, 'fog': 1})

# Create lag features
lag_features = ['T2','U','V','RH2','P_sfc']  # Replace with your actual feature names
for feature in lag_features:
    for lag in range(1, 2):  # Creating lag features for the past 2 time steps
        data1[f'{feature}_lag_{lag}'] = data1[feature].shift(lag)
# Define the frequency of your data
lag_hours = 1

# Create a lagged feature for 'class_vis'
#data1['class_vis_lag_24hr'] = data1['Vis'].shift(lag_hours)

# Drop rows with missing values created by lag features
data1 = data1.dropna()

# Split data into features and target
X1 = data1.drop(['Time', 'class_vis', 'Vis'], axis=1)  # Dropping 'Time' as it is now decomposed into useful features
y1 = data1['class_vis']

# Feature scaling
scaler = StandardScaler()
X_new_scaled = scaler.fit_transform(X1)

# Predictions
y_new_prob = et_classifier.predict_proba(X_new_scaled)
y_new_pred = (y_new_prob[:, 1] >= 0.55).astype(int)

## Visualizing the predictions of 2024

# Evaluation on Test Set
test_accuracy = accuracy_score(y1, y_new_pred)
test_precision = precision_score(y1, y_new_pred, average='weighted')
test_recall = recall_score(y1, y_new_pred, average='weighted')
test_f1 = f1_score(y1, y_new_pred, average='weighted')

print(f"Test Set Evaluation:\nAccuracy: {test_accuracy:.4f}")
print(f"Precision: {test_precision:.4f}")
print(f"Recall: {test_recall:.4f}")
print(f"F1-Score: {test_f1:.4f}")
print("\nClassification Report:\n", classification_report(y1, y_new_pred))

# Visualizing the Confusion Matrix
conf_matrix = confusion_matrix(y1, y_new_pred)
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['clear', 'fog'], yticklabels=['clear', 'fog'])
plt.title('Confusion Matrix for Yarmouth 2024 (ET)')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# --- Feature Importance Plot ---
importances = et_classifier.feature_importances_
feature_names = X1.columns

# Put into a DataFrame for sorting
feat_importances = pd.DataFrame({
    "Feature": feature_names,
    "Importance": importances
}).sort_values(by="Importance", ascending=False)

# Plot
plt.figure(figsize=(10, 6))
sns.barplot(x="Importance", y="Feature", data=feat_importances)
plt.title("Feature Importance - ETClassifier (Yarmouth 2024)")
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

# adding the pred values to a csv file

# Add predictions to the original dataframe
data1['Predicted_class_vis'] = y_new_pred

# Save the dataframe with predictions to a new CSV file
output_df = data1[['Time', 'Vis', 'class_vis', 'Predicted_class_vis']]
output_df.to_csv('yar2024_predictions_cleaned.csv', index=False)

import pandas as pd

# Load both files
df = pd.read_csv("yar2024_predictions_cleaned.csv")
dfyar = pd.read_csv("WRFtestYarmouth2024.csv")
dfyar_cleaned = dfyar.drop(index=0)

# Map WRF class to binary
dfyar_cleaned['class_visWRF_binary'] = dfyar_cleaned['class_visWRF'].str.strip().str.lower().map({'clear': 0, 'fog': 1})

# Now assign it to the main dataframe
df['class_visWRF_binary'] = dfyar_cleaned['class_visWRF_binary'].values

# Save
df.to_csv("yar2024_predictions_withWRF.csv", index=False)

# Load merged CSV
df = pd.read_csv("yar2024_predictions_withWRF.csv")

# Convert Time to datetime
df["Time"] = pd.to_datetime(df["Time"], errors="coerce")

# --- Select 5-day window ---
start_date = "2024-07-05"
end_date = "2024-07-10"

df_subset = df[(df["Time"] >= start_date) & (df["Time"] <= end_date)]

# --- Plot ---
plt.figure(figsize=(12, 5))
plt.plot(df_subset["Time"], df_subset["class_vis"], label="Observed", color="black", linewidth=2)
plt.plot(df_subset["Time"], df_subset["Predicted_class_vis"], label="Predicted", color="red", linestyle="--")
plt.plot(df_subset["Time"], df_subset["class_visWRF_binary"], label="WRF", color="blue", linestyle=":")

plt.title(f"Fog Case Study ({start_date} to {end_date}) for Yarmouth", fontsize=14)
plt.xlabel("Time (UTC)", fontsize=12)
plt.ylabel("Fog / Clear", fontsize=12)
plt.yticks([0, 1], ["Clear", "Fog"])  # ðŸ‘ˆ Restrict y-axis labels
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

import pandas as pd
from sklearn.metrics import f1_score
import matplotlib.pyplot as plt

# Load the prediction file
output_df = pd.read_csv('yar2024_predictions_cleaned.csv')
output_df['Time'] = pd.to_datetime(output_df['Time'])
output_df['Hour'] = output_df['Time'].dt.hour

# Define time bins
bins = [0, 6, 12, 18, 24]
labels = ['00-05h', '06-11h', '12-17h', '18-23h']
output_df['Time_Bin'] = pd.cut(output_df['Hour'], bins=bins, labels=labels, right=False)

# F1-score for fog (class 1)
f1_fog_per_bin = output_df.groupby('Time_Bin').apply(
    lambda df: f1_score(df['class_vis'], df['Predicted_class_vis'], pos_label=1, average='binary')
)

# Count of fog cases in each bin (class_vis == 1)
fog_counts = output_df[output_df['class_vis'] == 1].groupby('Time_Bin').size()

# Plotting
ax = f1_fog_per_bin.plot(kind='bar', color='skyblue', edgecolor='black')
plt.title('Fog Detection F1-Score by 6-Hour Time Bins (Yarmouth)')
plt.xlabel('Time Bin')
plt.ylabel('F1-Score (Fog Only)')
plt.ylim(0, 1)
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Add fog count text above bars
for i, bin_label in enumerate(f1_fog_per_bin.index):
    count = fog_counts.get(bin_label, 0)
    ax.text(i, f1_fog_per_bin[i] + 0.03, f'{count} fog', ha='center', va='bottom', fontsize=9)

plt.tight_layout()
plt.show()

# @title Doing confidence intervals and mcnemar test (Yarmouth)
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
from statsmodels.stats.contingency_tables import mcnemar

# --- Load your CSV ---
dfyar = pd.read_csv("WRFtestYarmouth2024.csv")
dfyar_cleaned = dfyar.drop(index=0)
dfyarpred = pd.read_csv("yar2024_predictions_cleaned.csv")


# Binary classification: Map 'class_vis' to binary classes
dfyar_cleaned['Vis'] = dfyar_cleaned['Vis'].map({'clear': 0, 'fog': 1})
dfyar_cleaned['class_visWRF'] = dfyar_cleaned['class_visWRF'].map({'clear': 0, 'fog': 1})

# Assuming columns:
#   - 'true_label' for the ground truth
#   - 'class_visWRF' for WRF predictions
#   - 'class_visET'  for ExtraTrees predictions (adjust column name if different)
y_true = dfyar_cleaned["Vis"].values
y_pred_wrf = dfyar_cleaned["class_visWRF"].values
y_pred_et  = dfyarpred["Predicted_class_vis"]

# -----------------------------
# 1. McNemarâ€™s Test
# -----------------------------
b = np.sum((y_pred_wrf == y_true) & (y_pred_et != y_true))
c = np.sum((y_pred_wrf != y_true) & (y_pred_et == y_true))

table = [[0, b],
         [c, 0]]

result = mcnemar(table, exact=False, correction=True)
print(f"McNemarâ€™s test statistic: {result.statistic}, p-value: {result.pvalue}")

# -----------------------------
# 2. Bootstrap Confidence Intervals
# -----------------------------
def bootstrap_ci(y_true, y_pred, metric, n_boot=10000, alpha=0.05):
    scores = []
    n = len(y_true)
    for _ in range(n_boot):
        idx = np.random.randint(0, n, n)
        y_true_resampled = y_true[idx]
        y_pred_resampled = y_pred[idx]
        scores.append(metric(y_true_resampled, y_pred_resampled))
    lower = np.percentile(scores, 100*alpha/2)
    upper = np.percentile(scores, 100*(1-alpha/2))
    return np.mean(scores), (lower, upper)

metrics = {
    "Precision": precision_score,
    "Recall": recall_score,
    "F1": f1_score,
    "Accuracy": accuracy_score
}

print("\nETClassifier metrics with 95% CIs:")
for name, metric in metrics.items():
    mean, (ci_low, ci_high) = bootstrap_ci(y_true, y_pred_et, metric)
    print(f"{name}: {mean:.3f} (95% CI: {ci_low:.3f} â€“ {ci_high:.3f})")

# @title Testing with new data (St John's)
# Create a path and read the new data with pd
path1 = 'testStjohn2024_1KM.csv'

read1 = pd.read_csv(path1)
# Remove any blanks
read1 = read1[read1['Vis'].notnull()]
# Save to a csv file
read1.to_csv('cleaned1_file.csv', index=False)

## Data Preparation

# Convert 'Time' column to datetime with the correct format
data1 = read1.copy()
data1['Time'] = pd.to_datetime(data1['Time'], format='%Y-%m-%d_%H:%M:%S', errors='coerce')

# Extract time-based features
data1['Hour'] = data1['Time'].dt.hour
#data1['Day'] = data1['Time'].dt.day
data1['Month'] = data1['Time'].dt.month
#data1['sin_hour'] = np.sin(2 * np.pi * data1['Hour'] / 24)

# Sort the data by time
data1 = data1.sort_values(by='Time')

data1 = data1[data1['Month'] > 3]

# Binary classification: Map 'class_vis' to binary classes
data1['class_vis'] = data1['class_vis'].map({'clear': 0, 'fog': 1})

# Create lag features
lag_features = ['T2','U','V','RH2','P_sfc']  # Replace with your actual feature names
for feature in lag_features:
    for lag in range(1, 2):  # Creating lag features for the past 2 time steps
        data1[f'{feature}_lag_{lag}'] = data1[feature].shift(lag)
# Define the frequency of your data
lag_hours = 1

# Create a lagged feature for 'class_vis'
#data1['class_vis_lag_24hr'] = data1['Vis'].shift(lag_hours)

# Drop rows with missing values created by lag features
data1 = data1.dropna()

# Split data into features and target
X1 = data1.drop(['Time', 'class_vis', 'Vis'], axis=1)  # Dropping 'Time' as it is now decomposed into useful features
y1 = data1['class_vis']

# Feature scaling
scaler = StandardScaler()
X_new_scaled = scaler.fit_transform(X1)

# Predictions
y_new_prob = et_classifier.predict_proba(X_new_scaled)
y_new_pred = (y_new_prob[:, 1] >= 0.65).astype(int)

## Visualizing the predictions of 2024

# Evaluation on Test Set
test_accuracy = accuracy_score(y1, y_new_pred)
test_precision = precision_score(y1, y_new_pred, average='weighted')
test_recall = recall_score(y1, y_new_pred, average='weighted')
test_f1 = f1_score(y1, y_new_pred, average='weighted')

print(f"Test Set Evaluation:\nAccuracy: {test_accuracy:.4f}")
print(f"Precision: {test_precision:.4f}")
print(f"Recall: {test_recall:.4f}")
print(f"F1-Score: {test_f1:.4f}")
print("\nClassification Report:\n", classification_report(y1, y_new_pred))

# Visualizing the Confusion Matrix
conf_matrix = confusion_matrix(y1, y_new_pred)
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['clear', 'fog'], yticklabels=['clear', 'fog'])
plt.title("Confusion Matrix for St John's 2024")
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# --- Feature Importance Plot ---
importances = et_classifier.feature_importances_
feature_names = X1.columns

# Put into a DataFrame for sorting
feat_importances = pd.DataFrame({
    "Feature": feature_names,
    "Importance": importances
}).sort_values(by="Importance", ascending=False)

# Plot
plt.figure(figsize=(10, 6))
sns.barplot(x="Importance", y="Feature", data=feat_importances)
plt.title("Feature Importance - ETClassifier (St John's 2024)")
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

# adding the pred values to a csv file

# Add predictions to the original dataframe
data1['Predicted_class_vis'] = y_new_pred

# Save the dataframe with predictions to a new CSV file
output_df = data1[['Time', 'Vis', 'class_vis', 'Predicted_class_vis']]
output_df.to_csv('stjohn2024_predictions_cleaned.csv', index=False)

import pandas as pd
import matplotlib.pyplot as plt

# Load both files
df = pd.read_csv("stjohn2024_predictions_cleaned.csv")
dfstj = pd.read_csv("WRF_stjohn2024.csv")
dfstj_cleaned = dfstj.drop(index=0)

# Map WRF class to binary
dfstj_cleaned['class_visWRF_binary'] = (
    dfstj_cleaned['class_visWRF']
    .str.strip()
    .str.lower()
    .map({'clear': 0, 'fog': 1})
)

# Now assign it to the main dataframe
df['class_visWRF_binary'] = dfstj_cleaned['class_visWRF_binary'].values

# Save merged CSV
df.to_csv("stjohns2024_predictions_withWRF.csv", index=False)

# Load merged CSV
df = pd.read_csv("stjohns2024_predictions_withWRF.csv")

# Convert Time to datetime
df["Time"] = pd.to_datetime(df["Time"], errors="coerce")

# --- Select 5-day window ---
start_date = "2024-05-10"
end_date = "2024-05-15"
df_subset = df[(df["Time"] >= start_date) & (df["Time"] <= end_date)]

# --- Plot ---
plt.figure(figsize=(12, 5))
plt.plot(df_subset["Time"], df_subset["class_vis"], label="Observed", color="black", linewidth=2)
plt.plot(df_subset["Time"], df_subset["Predicted_class_vis"], label="Predicted (ETClassifier)", color="red", linestyle="--")
plt.plot(df_subset["Time"], df_subset["class_visWRF_binary"], label="WRF", color="blue", linestyle=":")

plt.title(f"Fog Case Study ({start_date} to {end_date}) for St. John's", fontsize=14)
plt.xlabel("Time (UTC)", fontsize=12)
plt.ylabel("Fog / Clear", fontsize=12)
plt.yticks([0, 1], ["Clear", "Fog"])  # ðŸ‘ˆ Restrict y-axis labels
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

import pandas as pd
from sklearn.metrics import f1_score
import matplotlib.pyplot as plt

# Load the prediction file
output_df = pd.read_csv('stjohn2024_predictions_cleaned.csv')
output_df['Time'] = pd.to_datetime(output_df['Time'])
output_df['Hour'] = output_df['Time'].dt.hour

# Define time bins
bins = [0, 6, 12, 18, 24]
labels = ['00-05h', '06-11h', '12-17h', '18-23h']
output_df['Time_Bin'] = pd.cut(output_df['Hour'], bins=bins, labels=labels, right=False)

# F1-score for fog (class 1)
f1_fog_per_bin = output_df.groupby('Time_Bin').apply(
    lambda df: f1_score(df['class_vis'], df['Predicted_class_vis'], pos_label=1, average='binary')
)

# Count of fog cases in each bin (class_vis == 1)
fog_counts = output_df[output_df['class_vis'] == 1].groupby('Time_Bin').size()

# Plotting
ax = f1_fog_per_bin.plot(kind='bar', color='skyblue', edgecolor='black')
plt.title('Fog Detection F1-Score by 6-Hour Time Bins (St John\'s)')
plt.xlabel('Time Bin')
plt.ylabel('F1-Score (Fog Only)')
plt.ylim(0, 1)
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Add fog count text above bars
for i, bin_label in enumerate(f1_fog_per_bin.index):
    count = fog_counts.get(bin_label, 0)
    ax.text(i, f1_fog_per_bin[i] + 0.03, f'{count} fog', ha='center', va='bottom', fontsize=9)

plt.tight_layout()
plt.show()

# @title Doing confidence intervals and mcnemar test (St John's)
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
from statsmodels.stats.contingency_tables import mcnemar

# --- Load your CSV ---
df3 = pd.read_csv("WRF_stjohn2024.csv")
df3_cleaned = df3.drop(index=0)
df4 = pd.read_csv("stjohn2024_predictions_cleaned.csv")


# Binary classification: Map 'class_vis' to binary classes
df3_cleaned['Vis'] = df3_cleaned['Vis'].map({'clear': 0, 'fog': 1})
df3_cleaned['class_visWRF'] = df3_cleaned['class_visWRF'].map({'clear': 0, 'fog': 1})

# Assuming columns:
#   - 'true_label' for the ground truth
#   - 'class_visWRF' for WRF predictions
#   - 'class_visET'  for ExtraTrees predictions (adjust column name if different)
y_true = df3_cleaned["Vis"].values
y_pred_wrf = df3_cleaned["class_visWRF"].values
y_pred_et  = df4["Predicted_class_vis"]

# -----------------------------
# 1. McNemarâ€™s Test
# -----------------------------
b = np.sum((y_pred_wrf == y_true) & (y_pred_et != y_true))
c = np.sum((y_pred_wrf != y_true) & (y_pred_et == y_true))

table = [[0, b],
         [c, 0]]

result = mcnemar(table, exact=False, correction=True)
print(f"McNemarâ€™s test statistic: {result.statistic}, p-value: {result.pvalue}")

# -----------------------------
# 2. Bootstrap Confidence Intervals
# -----------------------------
def bootstrap_ci(y_true, y_pred, metric, n_boot=10000, alpha=0.05):
    scores = []
    n = len(y_true)
    for _ in range(n_boot):
        idx = np.random.randint(0, n, n)
        y_true_resampled = y_true[idx]
        y_pred_resampled = y_pred[idx]
        scores.append(metric(y_true_resampled, y_pred_resampled))
    lower = np.percentile(scores, 100*alpha/2)
    upper = np.percentile(scores, 100*(1-alpha/2))
    return np.mean(scores), (lower, upper)

metrics = {
    "Precision": precision_score,
    "Recall": recall_score,
    "F1": f1_score,
    "Accuracy": accuracy_score
}

print("\nETClassifier metrics with 95% CIs:")
for name, metric in metrics.items():
    mean, (ci_low, ci_high) = bootstrap_ci(y_true, y_pred_et, metric)
    print(f"{name}: {mean:.3f} (95% CI: {ci_low:.3f} â€“ {ci_high:.3f})")

# @title WRF 2024 results

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score

# Define true labels (observations) and predicted labels (WRF model)
y_true = np.array([1]*413 + [0]*198 + [1]*170 + [0]*2890)  # 1 = fog, 0 = clear
y_pred = np.array([1]*413 + [1]*198 + [0]*170 + [0]*2890)  # 1 = fog, 0 = clear

# Compute confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Compute precision, recall, and F1-score
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)

# Plot confusion matrix
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["clear", "fog"], yticklabels=["clear", "fog"],  annot_kws={"size": 12})
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix for St John's (ETClassifier)")

# Print metrics
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1-score: {f1:.2f}")

plt.show()

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Define confusion matrix values
cm = np.array([[2866, 222],  # Clear (Actual)
               [221, 362]])  # Fog (Actual)

# Plot confusion matrix
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Clear", "Fog"], yticklabels=["Clear", "Fog"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix for St. John's (WRF Model)")

plt.show()

import pandas as pd
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score, f1_score, classification_report

# Load your CSV file
df = pd.read_csv("WRF_stjohn2024.csv")

# Assuming your columns are named 'true' and 'predicted'
y_true = df['Vis']
y_pred = df['class_visWRF']

# Precision, Recall, F1 for "fog" only
precision = precision_score(y_true, y_pred, pos_label="fog")
recall = recall_score(y_true, y_pred, pos_label="fog")
f1 = f1_score(y_true, y_pred, pos_label="fog")

print(f"Precision (fog): {precision:.2f}")
print(f"Recall (fog): {recall:.2f}")
print(f"F1 Score (fog): {f1:.2f}")

# Or full classification report
print("\nClassification Report:")
print(classification_report(y_true, y_pred, labels=["fog", "clear"]))

# Compute confusion matrix
cm = confusion_matrix(y_true, y_pred, labels=['fog', 'clear'])

# Display
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['fog', 'clear'])
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix for WRF")
plt.show()

"""## All locations"""

# @title Cleaning the Data
# Load the CSV file into a DataFrame
# Load and clean data
def load_and_clean_data(path):
    df = pd.read_csv(path)
    df = df[df['Vis'].notnull()]
    return df


location1_data = load_and_clean_data('Era5_Stjohn.csv')
location2_data = load_and_clean_data('Era5_Yarmouth.csv')

# Ensure the time columns are in datetime format
location1_data['Time'] = pd.to_datetime(location1_data['Time'], format='%Y-%m-%d_%H:%M:%S')
location2_data['Time'] = pd.to_datetime(location2_data['Time'], format='%Y-%m-%d_%H:%M:%S')

# Add location identifiers
location1_data['Location'] = 'StJohn'
location2_data['Location'] = 'Yarmouth'

# Combine the datasets
combined_data = pd.concat([location1_data, location2_data])
combined_data.to_csv('combined_data.csv', index=False)

# One-hot encode the location column
combined_data = pd.get_dummies(combined_data, columns=['Location'], drop_first=False)

# Binary classification: Map 'class_vis' to binary classes (assuming 'clear' and 'fog' are the classes)
combined_data['class_vis'] = combined_data['class_vis'].map({'clear': 0, 'fog': 1})

# Extract time-based features
combined_data['Hour'] = combined_data['Time'].dt.hour
combined_data['Day'] = combined_data['Time'].dt.day
combined_data['Month'] = combined_data['Time'].dt.month
combined_data['DayOfWeek'] = combined_data['Time'].dt.dayofweek

#Sort the data by time
combined_data = combined_data.sort_values(by='Time')
combined_data = combined_data[combined_data['Month'] > 3]

# Drop the original time column
combined_data = combined_data.drop(['Time'], axis=1)

# Create lag features
lag_features = [col for col in combined_data.columns if col not in ['class_vis','T2-TD2','TD2','Vis','Hour','Day','Month','DayOfWeek','Location_StJohn','Location_Yarmouth']]
for feature in lag_features:
    for lag in range(1, 2):  # Creating lag features for the past 2 time steps
        combined_data[f'{feature}_lag_{lag}'] = combined_data[feature].shift(lag)

# Drop rows with missing values created by lag features
combined_data = combined_data.dropna()
# Preserve the locations data
location_columns = combined_data[['Location_StJohn', 'Location_Yarmouth']]

# Define features and target
X = combined_data.drop(['class_vis','Vis', 'T2-TD2','TD2'], axis=1)
y = combined_data['class_vis']

# Train-test split based on time
train_size = int(len(combined_data) * 0.9)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# Split the locations into train and test sets
locations_train = location_columns[:train_size]
locations_test = location_columns[train_size:]

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Apply SMOTE for data balancing
smote = BorderlineSMOTE()
X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)

# @title Find the parameters

# Define the parameter distribution for RandomizedSearchCV
param_dist = {
    'n_estimators': randint(100, 500),
    'criterion': ['gini', 'log_loss', 'entropy'],
    'max_features': ['sqrt', 'log2',None],
    'max_depth': [10, 20, 30, 40, None],
    'min_samples_split': randint(2, 10),
    'min_samples_leaf': randint(1, 5),
    'class_weight': ['balanced', 'balanced_subsample'],
    'bootstrap': [True, False]
}

# Initialize the Extra Trees classifier
et_classifier = ExtraTreesClassifier()

# Initialize RandomizedSearchCV
tscv = TimeSeriesSplit(n_splits=5)
random_search = RandomizedSearchCV(estimator=et_classifier, param_distributions=param_dist, scoring= make_scorer(f1_score, average='weighted'), n_iter=50, cv=tscv, n_jobs=-1, verbose=2)

# Fit the model
random_search.fit(X_train_res, y_train_res)

# Print the best parameters
print("Best Parameters:", random_search.best_params_)

# @title Building the Model
# Initialize and train ExtraTreeClassifier
et_classifier = ExtraTreesClassifier(
    n_estimators=475,  # Number of trees
    criterion = 'entropy',
    max_depth=40,      # Maximum depth of each tree
    max_features='log2',  # Number of features to consider when looking for the best split
    min_samples_split=5,  # Minimum number of samples required to split an internal node
    min_samples_leaf=3,   # Minimum number of samples required to be at a leaf node
    bootstrap= False,    # Whether bootstrap samples are used when building trees
    class_weight='balanced'
)
# Fit the model
et_classifier.fit(X_train_res, y_train_res)

# Predictions
y_train_pred = et_classifier.predict(X_train_scaled)
y_test_prob = et_classifier.predict_proba(X_test_scaled)
y_test_pred = (y_test_prob[:, 1] >= 0.6).astype(int)

# Evaluation on Training Set
train_accuracy = accuracy_score(y_train, y_train_pred)
train_precision = precision_score(y_train, y_train_pred, average='weighted')
train_recall = recall_score(y_train, y_train_pred, average='weighted')
train_f1 = f1_score(y_train, y_train_pred, average='weighted')

print(f"Training Set Evaluation:\nAccuracy: {train_accuracy:.4f}")
print(f"Precision: {train_precision:.4f}")
print(f"Recall: {train_recall:.4f}")
print(f"F1-Score: {train_f1:.4f}")

# Evaluation on Test Set
test_accuracy = accuracy_score(y_test, y_test_pred)
test_precision = precision_score(y_test, y_test_pred, average='weighted')
test_recall = recall_score(y_test, y_test_pred, average='weighted')
test_f1 = f1_score(y_test, y_test_pred, average='weighted')

print(f"Test Set Evaluation:\nAccuracy: {test_accuracy:.4f}")
print(f"Precision: {test_precision:.4f}")
print(f"Recall: {test_recall:.4f}")
print(f"F1-Score: {test_f1:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_test_pred))

# Cross-Validation
cv_scores = cross_val_score(et_classifier, X, y, cv=5)
print(f"Cross-Validation Scores: {cv_scores}")
print(f"Mean CV Accuracy: {cv_scores.mean():.4f}")

# Feature Importance
importances = et_classifier.feature_importances_
feature_names = X.columns
feature_importance_df = pd.DataFrame(importances, index=feature_names, columns=['Importance']).sort_values('Importance', ascending=False)
print("\nFeature Importance:\n", feature_importance_df)

# Visualization of Feature Importance
plt.figure(figsize=(10, 6))
sns.barplot(x=feature_importance_df['Importance'], y=feature_importance_df.index)
plt.title('Feature Importance')
plt.show()

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['clear', 'fog'], yticklabels=['clear', 'fog'])
plt.title('Confusion Matrix for St Johns and Yarmouth combined')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# @title Evaluate the Perfomance
# Create a DataFrame with the scaled features and predicted values
X_test_df = pd.DataFrame(X_test_scaled, columns=X_train.columns)
X_test_df = X_test_df.reset_index(drop=True)  # Reset index for X_test_df
test_results = X_test_df.copy()
test_results['True'] = y_test.reset_index(drop=True)
test_results['Predicted'] = y_test_pred

# One-hot encode the location column
location_columns = ['Location_StJohn', 'Location_Yarmouth']

# Ensure test_results includes location columns from the original test set
test_results[location_columns] = X_test[location_columns].reset_index(drop=True)

# Group by locations and print the accuracy for each location
for location_col in location_columns:
    location_accuracy = test_results[test_results[location_col] == 1].apply(lambda x: x['True'] == x['Predicted'], axis=1).mean()
    location_name = location_col.split('_')[1]
    print(f"Accuracy for {location_name}: {location_accuracy:.4f}")

# Create classification reports for each location
for location_col in location_columns:
    location_name = location_col.split('_')[1]
    location_test_results = test_results[test_results[location_col] == 1]
    print(f"\nClassification Report for {location_name}:\n")
    print(classification_report(location_test_results['True'], location_test_results['Predicted']))

# Create confusion matrices for each location
for location_col in location_columns:
    location_name = location_col.split('_')[1]
    location_test_results = test_results[test_results[location_col] == 1]
    location_conf_matrix = confusion_matrix(location_test_results['True'], location_test_results['Predicted'])

    plt.figure(figsize=(10, 7))
    sns.heatmap(location_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['clear', 'fog'], yticklabels=['clear', 'fog'])
    plt.title(f'Confusion Matrix for {location_name}  (Combined locations)')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

"""## Multi-Class"""

# @title Cleaning the Data
# Load the CSV file into a DataFrame
# Define the path and pattern for the CSV files
path = 'Era5multi_Stjohn.csv'

# Compute the Dask DataFrame to get a Pandas DataFrame
combined_df = pd.read_csv(path)
# Remove rows where the first column is blank
df_cleaned = combined_df[combined_df['Vis'].notnull()]

# Save the cleaned DataFrame back to a CSV file
df_cleaned.to_csv('cleaned_file.csv', index=False)

# @title Data Preparation
# Convert 'Time' column to datetime with the correct format
data = df_cleaned.copy()
data['Time'] = pd.to_datetime(data['Time'], format='%Y-%m-%d_%H:%M:%S', errors='coerce')

# Extract time-based features
data['Hour'] = data['Time'].dt.hour
data['Day'] = data['Time'].dt.day
data['Month'] = data['Time'].dt.month

# Sort the data by time
data = data.sort_values(by='Time')

data = data[data['Month'] > 3]
#data['Wind_Speed'] = (data['U']**2 + data['V']**2) ** 0.5

# Multi classification: Map 'class_vis' to multi classes
label_encoder = LabelEncoder()
data['class_vis'] = label_encoder.fit_transform(data['class_vis'])

# Create lag features
lag_features = ['T2','U','V','RH2','P_sfc', 'T2-TD2']  # feature names
for feature in lag_features:
    for lag in range(1, 2):  # Creating lag features for the past 2 time steps
        data[f'{feature}_lag_{lag}'] = data[feature].shift(lag)

# Drop rows with missing values created by lag features
data = data.dropna()

# Split data into features and target
X = data.drop(['Time', 'class_vis', 'Vis','TD2'], axis=1)  # Dropping 'Time' as it is now decomposed into useful features
y = data['class_vis']

# Train-test split based on time
train_size = int(len(data) * 0.9)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Apply SMOTE for data balancing
from imblearn.combine import SMOTEENN
from imblearn.over_sampling import BorderlineSMOTE
smote= SMOTEENN()
X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)
from imblearn.under_sampling import RandomUnderSampler
# Counting the labels
unique, counts = np.unique(y_train, return_counts=True)
print(dict(zip(unique, counts)))
# Define undersampling strategy
undersample = RandomUnderSampler(sampling_strategy={0: 10000, 1: 4259, 2: 2414}, random_state=42)

# Apply undersampling
#X_train_res, y_train_res = undersample.fit_resample(X_train, y_train)

# Check new class distribution
unique_resampled, counts_resampled = np.unique(y_train_res, return_counts=True)
print(dict(zip(unique_resampled, counts_resampled)))

# @title Find the parameters for St John

# Define the parameter distribution for RandomizedSearchCV
param_dist = {
    'n_estimators': randint(100, 500),
    'criterion': ['gini', 'log_loss', 'entropy'],
    'max_features': ['sqrt', 'log2', None],
    'max_depth': [10, 20, 30, 40, None],
    'min_samples_split': randint(2, 15),
    'min_samples_leaf': randint(1, 5),
    'class_weight': ['balanced', 'balanced_subsample', None],
    'bootstrap': [True, False]
}

# Initialize the Extra Trees classifier
et_classifier = ExtraTreesClassifier()

# Initialize RandomizedSearchCV
random_search = RandomizedSearchCV(estimator=et_classifier, param_distributions=param_dist, scoring= f1_score, n_iter=50, cv=5, n_jobs=-1, verbose=2)

# Fit the model
random_search.fit(X_train_res, y_train_res)

# Print the best parameters
print("Best Parameters:", random_search.best_params_)

# @title Building the Model (St John)
# Initialize and train ExtraTreeClassifier
et_classifier = ExtraTreesClassifier(
    n_estimators=300,   # Number of trees
    criterion = 'gini',
    max_depth= None,      # Maximum depth of each tree
    max_features='sqrt',  # Number of features to consider when looking for the best split
    min_samples_split=14,  # Minimum number of samples required to split an internal node
    min_samples_leaf=2,   # Minimum number of samples required to be at a leaf node
    bootstrap= True,    # Whether bootstrap samples are used when building trees
    class_weight= 'balanced_subsample',
    random_state=42
)
# Fit the model
et_classifier.fit(X_train_res, y_train_res)

# Predictions
y_train_pred = et_classifier.predict(X_train_scaled)
y_test_prob = et_classifier.predict_proba(X_test_scaled)
y_test_pred = (y_test_prob[:, 1] >= 0.5).astype(int)

# Evaluation on Training Set
train_accuracy = accuracy_score(y_train, y_train_pred)
train_precision = precision_score(y_train, y_train_pred, average='weighted')
train_recall = recall_score(y_train, y_train_pred, average='weighted')
train_f1 = f1_score(y_train, y_train_pred, average='weighted')

print(f"Training Set Evaluation:\nAccuracy: {train_accuracy:.4f}")
print(f"Precision: {train_precision:.4f}")
print(f"Recall: {train_recall:.4f}")
print(f"F1-Score: {train_f1:.4f}")

# Evaluation on Test Set
test_accuracy = accuracy_score(y_test, y_test_pred)
test_precision = precision_score(y_test, y_test_pred, average='weighted')
test_recall = recall_score(y_test, y_test_pred, average='weighted')
test_f1 = f1_score(y_test, y_test_pred, average='weighted')

print(f"Test Set Evaluation:\nAccuracy: {test_accuracy:.4f}")
print(f"Precision: {test_precision:.4f}")
print(f"Recall: {test_recall:.4f}")
print(f"F1-Score: {test_f1:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_test_pred))

# Cross-Validation
cv_scores = cross_val_score(et_classifier, X, y, cv=5)
print(f"Cross-Validation Scores: {cv_scores}")
print(f"Mean CV Accuracy: {cv_scores.mean():.4f}")

# Feature Importance
importances = et_classifier.feature_importances_
feature_names = X.columns
feature_importance_df = pd.DataFrame(importances, index=feature_names, columns=['Importance']).sort_values('Importance', ascending=False)
print("\nFeature Importance:\n", feature_importance_df)

# Visualization of Feature Importance
plt.figure(figsize=(10, 6))
sns.barplot(x=feature_importance_df['Importance'], y=feature_importance_df.index)
plt.title('Feature Importance (Multi-class)')
plt.show()

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['clear', 'fog', 'mist'], yticklabels=['clear','fog','mist'])
plt.title('Confusion Matrix (Multi-class)')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

"""# Regression"""

# @title Cleaning the Data
# Load the CSV file into a DataFrame
# Define the path and pattern for the CSV files
path = 'Era5_Stjohn.csv'

# Compute the Dask DataFrame to get a Pandas DataFrame
combined_df = pd.read_csv(path)
# Remove rows where the first column is blank
df_cleaned = combined_df[combined_df['Vis'].notnull()]

# Save the cleaned DataFrame back to a CSV file
df_cleaned.to_csv('cleaned_file.csv', index=False)

# @title Preparing the data
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Load the dataset
df = pd.read_csv("cleaned_file.csv")

# Convert Time column to datetime and extract features
df['Time'] = pd.to_datetime(df['Time'], format='%Y-%m-%d_%H:%M:%S', errors='coerce')
df['hour'] = df['Time'].dt.hour
df['dayofyear'] = df['Time'].dt.dayofyear
df['month'] = df['Time'].dt.month

#clipping the Vis values to 5 km
df['Vis'] = df['Vis'].clip(upper=10)
# Log transform the target variable Vis
df['Vis'] = np.log1p(df['Vis'])  # log(Vis + 1) to handle zero cases

# Create lagged features except for Vis
lag_features = ['T2', 'TD2', 'T2-TD2', 'U', 'V', 'RH2', 'P_sfc']
for feature in lag_features:
    df[f'{feature}_lag1'] = df[feature].shift(1)

df.dropna(inplace=True)  # Remove rows with NaN values from lagging

# Drop unnecessary columns
df_reg = df.drop(columns=["class_vis"])

# Split into train (2012-2022) and test (2023) sets
df_train = df_reg[df_reg['Time'].dt.year < 2023]
df_test = df_reg[df_reg['Time'].dt.year == 2023]

# Separate features and target
X_train = df_train.drop(columns=["Time", "Vis"])
y_train = df_train["Vis"]
X_test = df_test.drop(columns=["Time", "Vis"])
y_test = df_test["Vis"]

# Standardize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# @title Initialize Extra Trees Regressor
et_regressor = ExtraTreesRegressor(random_state=42)

# Define parameter grid for RandomizedSearchCV
param_dist = {
    'n_estimators': [350, 400, 450, 500],
    'criterion': ['squared_error', 'friedman_mse', 'poisson', 'mean_absolute_error'],
    'max_depth': [30, 40, 50, None],
    'min_samples_split': [2, 4, 5, 10],
    'min_samples_leaf': [2, 3, 4, 5],
    'max_features': ['sqrt', 'log2', None],
    'bootstrap': [True, False]

}

# Perform Randomized Search with TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=5)
random_search = RandomizedSearchCV(estimator=et_regressor, param_distributions=param_dist,
                                   n_iter=20, scoring='r2', cv=tscv, n_jobs=-1, verbose=2, random_state=42)
random_search.fit(X_train, y_train)

# Best parameters
best_params = random_search.best_params_
print("Best Parameters:", best_params)

# @title Train the best model
# Train the best model with sample weights
log_fog_threshold = np.log1p(1.2)  # log(1 + x)
sample_weights = np.where(y_train < log_fog_threshold, 5, 1) # Higher weight for low visibility cases
best_et_regressor = ExtraTreesRegressor(
    n_estimators= 450,  # 450
    criterion= 'squared_error', # squared_error
    max_depth= 50,  # 50
    min_samples_split= 2, # 2
    min_samples_leaf= 3, # 3
    bootstrap= False, # False
    max_features= 'log2', #log2
    random_state=42)

best_et_regressor.fit(X_train, y_train, sample_weight= sample_weights)

# Make predictions
y_pred = best_et_regressor.predict(X_test)

# Looking at the results for ETRegressor
# Inverse log transform predictions
y_pred1 = np.expm1(y_pred)
y_test1 = np.expm1(y_test)

# Evaluate model performance
mae = mean_absolute_error(y_test1, y_pred1)
mse = mean_squared_error(y_test1, y_pred1)
r2 = r2_score(y_test1, y_pred1)

print(f"MAE: {mae}")
print(f"MSE: {mse}")
print(f"R2 Score: {r2}")

# Plot first 24 hours of predictions vs actual
plt.figure(figsize=(10, 5))
plt.plot(df_test['Time'].iloc[4000:4072], y_test1.iloc[4000:4072], label='Actual', marker='o')
plt.plot(df_test['Time'].iloc[4000:4072], y_pred1[4000:4072], label='Predicted', marker='x')

# Add a horizontal red line at y = 1.2 km to indicate fog threshold
plt.axhline(y=1.2, color='r', linestyle='--', label='Fog Threshold (1.2 km)')

plt.xlabel('Time')
plt.ylabel('Visibility')
plt.title('72 Hours: Predicted vs Actual Visibility')
plt.legend()
plt.xticks(rotation=45)
plt.grid()
plt.show()

# @title Quantile Regression Forest
from sklearn.ensemble import RandomForestRegressor
# Initialize Quantile Regression Forest (RandomForestRegressor with quantile support)
qrf = RandomForestRegressor(random_state=42)

# Define parameter grid for RandomizedSearchCV
param_dist = {
    'n_estimators': [50, 100, 200, 300, 400],
    'max_depth': [None, 10, 20, 30, 50],
    'min_samples_split': [2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4, 8]
}

# Perform Randomized Search with TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=5)
random_search = RandomizedSearchCV(estimator=qrf, param_distributions=param_dist,
                                   n_iter=20, scoring='r2', cv=tscv, n_jobs=-1, verbose=2, random_state=42)
random_search.fit(X_train, y_train)

# Best parameters
best_params = random_search.best_params_
print("Best Parameters:", best_params)

# @title Quantile Regression Forest with Vis clipped at 10km
from sklearn.ensemble import RandomForestRegressor
# Initialize Quantile Regression Forest (RandomForestRegressor with quantile support)
qrf = RandomForestRegressor(random_state=42)

# Define parameter grid for RandomizedSearchCV
param_dist = {
    'n_estimators': [50, 100, 200, 300, 400],
    'max_depth': [None, 10, 20, 30, 50],
    'min_samples_split': [2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4, 8]
}

# Perform Randomized Search with TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=5)
random_search = RandomizedSearchCV(estimator=qrf, param_distributions=param_dist,
                                   n_iter=20, scoring='r2', cv=tscv, n_jobs=-1, verbose=2, random_state=42)
random_search.fit(X_train, y_train)

# Best parameters
best_params = random_search.best_params_
print("Best Parameters:", best_params)

# Train the best model
from sklearn.ensemble import RandomForestRegressor

best_qrf = RandomForestRegressor(
    n_estimators= 200,
    max_depth= None,
    min_samples_split= 10,
    min_samples_leaf= 8,
    random_state=42)

best_qrf.fit(X_train, y_train)

# Predict quantiles
def predict_quantiles(model, X, quantiles=[0.05, 0.5, 0.95]):
    preds = np.array([tree.predict(X) for tree in model.estimators_])
    return {q: np.percentile(preds, q*100, axis=0) for q in quantiles}

quantiles = predict_quantiles(best_qrf, X_test)

# Inverse log transform predictions
y_pred_median = np.expm1(quantiles[0.5])
y_test = np.expm1(y_test)

# Evaluate model performance
mae = mean_absolute_error(y_test, y_pred_median)
mse = mean_squared_error(y_test, y_pred_median)
r2 = r2_score(y_test, y_pred_median)
rmse = np.sqrt(mse)

print(f"MAE: {mae}")
print(f"MSE: {mse}")
print(f"R2 Score: {r2}")
print(f"RMSE: {rmse}")

"""10 km clip:
1.   MAE: 1.1662956627340302
2.   MSE: 5.456176253968357
3.   R2 Score: 0.6081773926737035
4. RMSE: 2.335845939690449

2 km clip:
1. MAE: 0.1485630970389236
2. MSE: 0.11526540181271604
3. R2 Score: 0.6161231468563525
4. RMSE: 0.33950758726826125
"""

plt.figure(figsize=(10, 5))

# Actual Visibility
plt.plot(df_test['Time'].iloc[4000:4072], y_test.iloc[4000:4072], label='Actual', marker='o')

# Predicted Median Visibility
plt.plot(df_test['Time'].iloc[4000:4072], y_pred_median[4000:4072], label='Predicted Median', marker='x')

# 90% Prediction Interval (Shaded Region)
plt.fill_between(
    df_test['Time'].iloc[4000:4072],
    np.expm1(quantiles[0.05])[4000:4072],  # 5th percentile (lower bound)
    np.expm1(quantiles[0.95])[4000:4072],  # 95th percentile (upper bound)
    color='gray', alpha=0.3, label='90% Prediction Interval'
)
# Add a horizontal red line at y = 1.2 km to indicate fog threshold
plt.axhline(y=1.2, color='r', linestyle='--', label='Fog Threshold (1.2 km)')

plt.xlabel('Time/ UTC')
plt.ylabel('Visibility/ km')
plt.title('72 Hours: With Visibility clipped at 10 km (Quant. Regression)')
plt.legend(loc='upper left', bbox_to_anchor=(1, 1))
plt.xticks(rotation=45)
plt.grid()
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 5))

# Actual Visibility
plt.plot(df_test['Time'].iloc[4000:4072], y_test.iloc[4000:4072], label='Actual', marker='o')

# Predicted Median Visibility
plt.plot(df_test['Time'].iloc[4000:4072], y_pred_median[4000:4072], label='Predicted Median', marker='x')

# 90% Prediction Interval (Shaded Region)
plt.fill_between(
    df_test['Time'].iloc[4000:4072],
    np.expm1(quantiles[0.05])[4000:4072],  # 5th percentile (lower bound)
    np.expm1(quantiles[0.95])[4000:4072],  # 95th percentile (upper bound)
    color='gray', alpha=0.3, label='90% Prediction Interval'
)
# Add a horizontal red line at y = 1.2 km to indicate fog threshold
plt.axhline(y=1.2, color='r', linestyle='--', label='Fog Threshold (1.2 km)')

plt.xlabel('Time/ UTC')
plt.ylabel('Visibility/ km')
plt.title('72 Hours: With Visibility clipped at 2 km (Quant. Regression)')
plt.legend(loc='upper left', bbox_to_anchor=(1, 1))
plt.xticks(rotation=45)
plt.grid()
plt.tight_layout()
plt.show()